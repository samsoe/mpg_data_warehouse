{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fix NA Rows in gridVeg Additional Species\n",
        "\n",
        "This notebook investigates and fixes NA/NULL rows in the BigQuery table `mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_additional_species`.\n",
        "\n",
        "**Operation**: Identify and remove rows with NULL values in critical fields\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values for gridVeg additional species\n",
        "BQ_TABLE_ID = config['gridveg_additional_species']['bigquery']['table_id']\n",
        "BQ_PROJECT = config['gridveg_additional_species']['bigquery'].get('project')\n",
        "BACKUP_BUCKET = config['gridveg_additional_species']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gridveg_additional_species']['gcs'].get('backup_prefix', 'backups/gridveg_additional_species')\n",
        "\n",
        "# Verify required config values\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure gridveg_additional_species.bigquery.table_id in config.yml\")\n",
        "\n",
        "print(\"‚úì Configuration loaded successfully\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "\n",
        "print(f\"‚úì BigQuery client initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Investigate Current Table State\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get table schema and basic info\n",
        "table = bq_client.get_table(BQ_TABLE_ID)\n",
        "\n",
        "print(\"Table Schema:\")\n",
        "for field in table.schema:\n",
        "    print(f\"  {field.name}: {field.field_type} (nullable: {field.mode != 'REQUIRED'})\")\n",
        "\n",
        "print(f\"\\nTotal rows in table: {table.num_rows}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query to get all data from the table\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "\n",
        "print(\"Loading current table data...\")\n",
        "df_current = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"‚úì Data loaded: {len(df_current)} rows\")\n",
        "print(f\"  Columns: {list(df_current.columns)}\")\n",
        "\n",
        "# Display info\n",
        "df_current.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze NULL/NA Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for NULL values in each column\n",
        "print(\"NULL Value Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "null_counts = df_current.isnull().sum()\n",
        "null_percentages = (df_current.isnull().sum() / len(df_current) * 100)\n",
        "\n",
        "for col in df_current.columns:\n",
        "    null_count = null_counts[col]\n",
        "    null_pct = null_percentages[col]\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col:20s}: {null_count:5d} nulls ({null_pct:5.2f}%)\")\n",
        "    else:\n",
        "        print(f\"  {col:20s}: No nulls\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify rows with any NULL values\n",
        "rows_with_nulls = df_current[df_current.isnull().any(axis=1)]\n",
        "\n",
        "print(f\"Rows with at least one NULL value: {len(rows_with_nulls)}\")\n",
        "\n",
        "if len(rows_with_nulls) > 0:\n",
        "    print(f\"\\nBreakdown by column with NULL:\")\n",
        "    for col in df_current.columns:\n",
        "        null_in_col = df_current[df_current[col].isnull()]\n",
        "        if len(null_in_col) > 0:\n",
        "            print(f\"  {col}: {len(null_in_col)} rows\")\n",
        "    \n",
        "    print(f\"\\nSample of rows with NULL values:\")\n",
        "    display(rows_with_nulls.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check specifically for NULL in key_plant_species (the critical field)\n",
        "null_species = df_current[df_current['key_plant_species'].isnull()]\n",
        "\n",
        "print(f\"Rows with NULL key_plant_species: {len(null_species)}\")\n",
        "\n",
        "if len(null_species) > 0:\n",
        "    print(f\"\\nDistribution by year:\")\n",
        "    year_dist = null_species['year'].value_counts().sort_index()\n",
        "    for year, count in year_dist.items():\n",
        "        print(f\"  {year}: {count} rows\")\n",
        "    \n",
        "    print(f\"\\nSample records with NULL key_plant_species:\")\n",
        "    display(null_species.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA QUALITY ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTotal records in table: {len(df_current)}\")\n",
        "print(f\"Records with NULL values: {len(rows_with_nulls)} ({len(rows_with_nulls)/len(df_current)*100:.2f}%)\")\n",
        "print(f\"Clean records: {len(df_current) - len(rows_with_nulls)} ({(len(df_current) - len(rows_with_nulls))/len(df_current)*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nNULL values by column:\")\n",
        "for col in df_current.columns:\n",
        "    null_count = df_current[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col}: {null_count} ({null_count/len(df_current)*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backup Existing Table\n",
        "\n",
        "Before making any changes, create a backup of the existing table to GCS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backup existing table to GCS\n",
        "if BACKUP_BUCKET:\n",
        "    # Generate backup path with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = f\"gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/fix_na_rows_{timestamp}/*.csv\"\n",
        "    \n",
        "    print(f\"Creating backup of existing table...\")\n",
        "    print(f\"  Destination: {backup_path}\")\n",
        "    \n",
        "    # Export table to GCS\n",
        "    extract_job = bq_client.extract_table(\n",
        "        BQ_TABLE_ID,\n",
        "        backup_path,\n",
        "        location=\"US\"\n",
        "    )\n",
        "    \n",
        "    extract_job.result()  # Wait for job to complete\n",
        "    \n",
        "    print(f\"‚úì Backup completed successfully\")\n",
        "    print(f\"  Files: {backup_path}\")\n",
        "else:\n",
        "    print(\"‚ö† Backup bucket not configured in config.yml\")\n",
        "    print(\"  Set 'gridveg_additional_species.gcs.backup_bucket' to enable automatic backups\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Clean Data\n",
        "\n",
        "Remove rows with NULL values in key_plant_species field.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create clean dataset by removing rows with NULL key_plant_species\n",
        "df_clean = df_current[df_current['key_plant_species'].notna()].copy()\n",
        "\n",
        "print(\"Clean Dataset Preparation:\")\n",
        "print(f\"  Original rows:    {len(df_current)}\")\n",
        "print(f\"  Rows with NULL:   {len(df_current) - len(df_clean)}\")\n",
        "print(f\"  Clean rows:       {len(df_clean)}\")\n",
        "print(f\"  Rows to remove:   {len(df_current) - len(df_clean)}\")\n",
        "\n",
        "# Show what will be removed\n",
        "if len(df_current) - len(df_clean) > 0:\n",
        "    print(f\"\\nRows to be removed (by year):\")\n",
        "    removed_rows = df_current[df_current['key_plant_species'].isna()]\n",
        "    year_dist = removed_rows['year'].value_counts().sort_index()\n",
        "    for year, count in year_dist.items():\n",
        "        print(f\"  {year}: {count} rows\")\n",
        "\n",
        "# Verify data integrity\n",
        "print(f\"\\nData Integrity Check:\")\n",
        "print(f\"  NULL key_plant_species in clean data: {df_clean['key_plant_species'].isna().sum()}\")\n",
        "print(f\"  All rows have species?: {df_clean['key_plant_species'].notna().all()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace Table with Clean Data\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT**: This will REPLACE the entire table with the clean dataset (no NULL rows).\n",
        "\n",
        "Review the summary above before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace table with clean data\n",
        "print(\"=\" * 60)\n",
        "print(\"REPLACING BIGQUERY TABLE WITH CLEAN DATA\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTable: {BQ_TABLE_ID}\")\n",
        "print(f\"Current rows: {len(df_current)}\")\n",
        "print(f\"New rows (clean): {len(df_clean)}\")\n",
        "print(f\"Rows removed: {len(df_current) - len(df_clean)}\")\n",
        "print(f\"Mode: WRITE_TRUNCATE (replace entire table)\")\n",
        "print(f\"\\nStarting replacement at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "\n",
        "# Configure job to replace existing table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  # Replace entire table\n",
        ")\n",
        "\n",
        "# Load clean dataframe to BigQuery\n",
        "load_job = bq_client.load_table_from_dataframe(\n",
        "    df_clean,\n",
        "    BQ_TABLE_ID,\n",
        "    job_config=job_config\n",
        ")\n",
        "\n",
        "# Wait for job to complete\n",
        "load_job.result()\n",
        "\n",
        "print(f\"\\n‚úì Replacement completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"  Rows written: {load_job.output_rows}\")\n",
        "print(f\"  Job ID: {load_job.job_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Fix\n",
        "\n",
        "Read back the table to verify NA rows have been removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read updated table\n",
        "print(\"Verifying fix...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "df_updated = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"\\n‚úì Verification query complete\")\n",
        "print(f\"  Rows in table: {len(df_updated)}\")\n",
        "print(f\"  Columns: {list(df_updated.columns)}\")\n",
        "\n",
        "# Check for NULL values\n",
        "print(f\"\\nNULL Value Check:\")\n",
        "null_counts_after = df_updated.isnull().sum()\n",
        "for col in df_updated.columns:\n",
        "    null_count = null_counts_after[col]\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col}: {null_count} NULLs (‚ö†Ô∏è UNEXPECTED)\")\n",
        "    else:\n",
        "        print(f\"  {col}: No NULLs ‚úì\")\n",
        "\n",
        "# Show records by year\n",
        "print(f\"\\nRecords by year:\")\n",
        "year_counts = df_updated['year'].value_counts().sort_index()\n",
        "for year, count in year_counts.items():\n",
        "    print(f\"  {year}: {count} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify row counts\n",
        "expected_rows = len(df_clean)\n",
        "actual_rows = len(df_updated)\n",
        "\n",
        "print(\"\\nData integrity check:\")\n",
        "print(f\"  Expected rows:  {expected_rows}\")\n",
        "print(f\"  Actual rows:    {actual_rows}\")\n",
        "print(f\"  Rows removed:   {len(df_current) - actual_rows}\")\n",
        "\n",
        "if expected_rows == actual_rows:\n",
        "    print(f\"\\n‚úì Row count verified - table successfully cleaned\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† Row count mismatch!\")\n",
        "    print(f\"  Difference: {actual_rows - expected_rows}\")\n",
        "\n",
        "# Check if any NULL key_plant_species remain\n",
        "null_species_after = df_updated[df_updated['key_plant_species'].isna()]\n",
        "if len(null_species_after) == 0:\n",
        "    print(f\"\\n‚úì SUCCESS: No NULL key_plant_species values found in updated table\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† WARNING: {len(null_species_after)} NULL key_plant_species values still exist!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report\n",
        "\n",
        "Complete summary of the fix operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"FIX NA ROWS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\nüéØ Target:\")\n",
        "print(f\"  Table: {BQ_TABLE_ID}\")\n",
        "print(f\"  Project: {bq_client.project}\")\n",
        "\n",
        "print(f\"\\nüìä Data Changes:\")\n",
        "print(f\"  Original rows:  {len(df_current)}\")\n",
        "print(f\"  Cleaned rows:   {len(df_updated)}\")\n",
        "print(f\"  Rows removed:   {len(df_current) - len(df_updated)}\")\n",
        "\n",
        "if len(df_current) - len(df_updated) > 0:\n",
        "    removed_rows = df_current[df_current['key_plant_species'].isna()]\n",
        "    print(f\"\\n  Removed rows by year:\")\n",
        "    year_counts = removed_rows['year'].value_counts().sort_index()\n",
        "    for year, count in year_counts.items():\n",
        "        print(f\"    {year}: {count} rows\")\n",
        "\n",
        "print(f\"\\nüîÑ Operations Performed:\")\n",
        "print(f\"  ‚úì Backed up table to GCS\")\n",
        "print(f\"  ‚úì Removed rows with NULL key_plant_species\")\n",
        "print(f\"  ‚úì Replaced table with clean data\")\n",
        "print(f\"  ‚úì Verified data integrity\")\n",
        "\n",
        "if BACKUP_BUCKET:\n",
        "    print(f\"\\nüíæ Backup:\")\n",
        "    print(f\"  Location: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/\")\n",
        "    print(f\"  Status: ‚úì Created before fix\")\n",
        "\n",
        "# Final validation\n",
        "null_check = df_updated['key_plant_species'].isna().sum()\n",
        "if null_check == 0:\n",
        "    print(f\"\\n‚úÖ Fix completed successfully!\")\n",
        "    print(f\"   No NULL key_plant_species values remain in table\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è WARNING: {null_check} NULL values still exist\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollback Instructions (If Needed)\n",
        "\n",
        "If you need to rollback to the previous version, restore from the backup created at the beginning of this notebook.\n",
        "\n",
        "```python\n",
        "# To rollback, restore from backup:\n",
        "# backup_path = \"gs://BACKUP_BUCKET/BACKUP_PREFIX/fix_na_rows_TIMESTAMP/*.csv\"\n",
        "# df_backup = pd.read_csv(backup_path)\n",
        "# job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "# bq_client.load_table_from_dataframe(df_backup, BQ_TABLE_ID, job_config=job_config)\n",
        "```\n",
        "\n",
        "The backup location was printed in the backup cell above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mpg-data-warehouse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
