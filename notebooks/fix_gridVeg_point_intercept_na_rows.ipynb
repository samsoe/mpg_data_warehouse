{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fix NA Rows in gridVeg Point Intercept Vegetation\n",
        "\n",
        "This notebook investigates and fixes NA/NULL rows in the BigQuery table `mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_point_intercept_vegetation`.\n",
        "\n",
        "**Operation**: Identify and remove rows with NULL values in `intercept_ground_code` field\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n",
            "Libraries imported successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/esamsoe/miniforge3-new/envs/mpg-data-warehouse/lib/python3.9/site-packages/google/api_core/_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.23) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/Users/esamsoe/miniforge3-new/envs/mpg-data-warehouse/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Configuration loaded successfully\n",
            "  Table ID: mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_point_intercept_vegetation\n",
            "  Backup: gs://mpg-data-warehouse/gridVeg/bak\n"
          ]
        }
      ],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values for gridVeg point intercepts\n",
        "BQ_TABLE_ID = config['gridveg_point_intercepts']['bigquery']['table_vegetation']\n",
        "BQ_PROJECT = config['gridveg_point_intercepts']['bigquery'].get('project')\n",
        "BACKUP_BUCKET = config['gridveg_point_intercepts']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gridveg_point_intercepts']['gcs'].get('backup_prefix', 'backups/gridveg_point_intercepts')\n",
        "\n",
        "# Verify required config values\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure gridveg_point_intercepts.bigquery.table_vegetation in config.yml\")\n",
        "\n",
        "print(\"‚úì Configuration loaded successfully\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì BigQuery client initialized\n",
            "  Project: mpg-data-warehouse\n"
          ]
        }
      ],
      "source": [
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "\n",
        "print(f\"‚úì BigQuery client initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Investigate Current Table State\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table Schema:\n",
            "  survey_ID: STRING (nullable: True)\n",
            "  grid_point: INTEGER (nullable: True)\n",
            "  date: DATE (nullable: True)\n",
            "  year: INTEGER (nullable: True)\n",
            "  transect_point: STRING (nullable: True)\n",
            "  height_intercept_1: NUMERIC (nullable: True)\n",
            "  intercept_1: INTEGER (nullable: True)\n",
            "  intercept_2: INTEGER (nullable: True)\n",
            "  intercept_3: INTEGER (nullable: True)\n",
            "  intercept_4: INTEGER (nullable: True)\n",
            "\n",
            "Total rows in table: 298844\n"
          ]
        }
      ],
      "source": [
        "# Get table schema and basic info\n",
        "table = bq_client.get_table(BQ_TABLE_ID)\n",
        "\n",
        "print(\"Table Schema:\")\n",
        "for field in table.schema:\n",
        "    print(f\"  {field.name}: {field.field_type} (nullable: {field.mode != 'REQUIRED'})\")\n",
        "\n",
        "print(f\"\\nTotal rows in table: {table.num_rows}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading current table data...\n",
            "‚úì Data loaded: 298844 rows\n",
            "  Columns: ['survey_ID', 'grid_point', 'date', 'year', 'transect_point', 'height_intercept_1', 'intercept_1', 'intercept_2', 'intercept_3', 'intercept_4']\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 298844 entries, 0 to 298843\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count   Dtype \n",
            "---  ------              --------------   ----- \n",
            " 0   survey_ID           298844 non-null  object\n",
            " 1   grid_point          298844 non-null  Int64 \n",
            " 2   date                298844 non-null  dbdate\n",
            " 3   year                298844 non-null  Int64 \n",
            " 4   transect_point      298841 non-null  object\n",
            " 5   height_intercept_1  199493 non-null  object\n",
            " 6   intercept_1         298843 non-null  Int64 \n",
            " 7   intercept_2         97676 non-null   Int64 \n",
            " 8   intercept_3         19770 non-null   Int64 \n",
            " 9   intercept_4         2390 non-null    Int64 \n",
            "dtypes: Int64(6), dbdate(1), object(3)\n",
            "memory usage: 24.5+ MB\n"
          ]
        }
      ],
      "source": [
        "# Query to get all data from the table\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "\n",
        "print(\"Loading current table data...\")\n",
        "df_current = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"‚úì Data loaded: {len(df_current)} rows\")\n",
        "print(f\"  Columns: {list(df_current.columns)}\")\n",
        "\n",
        "# Display info\n",
        "df_current.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze NULL/NA Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NULL Value Analysis:\n",
            "============================================================\n",
            "  survey_ID           : No nulls\n",
            "  grid_point          : No nulls\n",
            "  date                : No nulls\n",
            "  year                : No nulls\n",
            "  transect_point      :     3 nulls ( 0.00%)\n",
            "  height_intercept_1  : 99351 nulls (33.25%)\n",
            "  intercept_1         :     1 nulls ( 0.00%)\n",
            "  intercept_2         : 201168 nulls (67.32%)\n",
            "  intercept_3         : 279074 nulls (93.38%)\n",
            "  intercept_4         : 296454 nulls (99.20%)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Check for NULL values in each column\n",
        "print(\"NULL Value Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "null_counts = df_current.isnull().sum()\n",
        "null_percentages = (df_current.isnull().sum() / len(df_current) * 100)\n",
        "\n",
        "for col in df_current.columns:\n",
        "    null_count = null_counts[col]\n",
        "    null_pct = null_percentages[col]\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col:20s}: {null_count:5d} nulls ({null_pct:5.2f}%)\")\n",
        "    else:\n",
        "        print(f\"  {col:20s}: No nulls\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify rows with any NULL values\n",
        "rows_with_nulls = df_current[df_current.isnull().any(axis=1)]\n",
        "\n",
        "print(f\"Rows with at least one NULL value: {len(rows_with_nulls)}\")\n",
        "\n",
        "if len(rows_with_nulls) > 0:\n",
        "    print(f\"\\nBreakdown by column with NULL:\")\n",
        "    for col in df_current.columns:\n",
        "        null_in_col = df_current[df_current[col].isnull()]\n",
        "        if len(null_in_col) > 0:\n",
        "            print(f\"  {col}: {len(null_in_col)} rows\")\n",
        "    \n",
        "    print(f\"\\nSample of rows with NULL values:\")\n",
        "    display(rows_with_nulls.head(20))\n",
        "\n",
        "# Check specifically for NULL in intercept_ground_code (the critical field)\n",
        "if 'intercept_ground_code' in df_current.columns:\n",
        "    null_ground_code = df_current[df_current['intercept_ground_code'].isnull()]\n",
        "    \n",
        "    print(f\"\\n\\nRows with NULL intercept_ground_code: {len(null_ground_code)}\")\n",
        "    \n",
        "    if len(null_ground_code) > 0:\n",
        "        print(f\"\\nSample records with NULL intercept_ground_code:\")\n",
        "        display(null_ground_code.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA QUALITY ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTotal records in table: {len(df_current)}\")\n",
        "print(f\"Records with NULL values: {len(rows_with_nulls)} ({len(rows_with_nulls)/len(df_current)*100:.2f}%)\")\n",
        "print(f\"Clean records: {len(df_current) - len(rows_with_nulls)} ({(len(df_current) - len(rows_with_nulls))/len(df_current)*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nNULL values by column:\")\n",
        "for col in df_current.columns:\n",
        "    null_count = df_current[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col}: {null_count} ({null_count/len(df_current)*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backup Existing Table\n",
        "\n",
        "Before making any changes, create a backup of the existing table to GCS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backup existing table to GCS\n",
        "if BACKUP_BUCKET:\n",
        "    # Generate backup path with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = f\"gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/fix_na_rows_{timestamp}/*.csv\"\n",
        "    \n",
        "    print(f\"Creating backup of existing table...\")\n",
        "    print(f\"  Destination: {backup_path}\")\n",
        "    \n",
        "    # Export table to GCS\n",
        "    extract_job = bq_client.extract_table(\n",
        "        BQ_TABLE_ID,\n",
        "        backup_path,\n",
        "        location=\"US\"\n",
        "    )\n",
        "    \n",
        "    extract_job.result()  # Wait for job to complete\n",
        "    \n",
        "    print(f\"‚úì Backup completed successfully\")\n",
        "    print(f\"  Files: {backup_path}\")\n",
        "else:\n",
        "    print(\"‚ö† Backup bucket not configured in config.yml\")\n",
        "    print(\"  Set 'gridveg_point_intercepts.gcs.backup_bucket' to enable automatic backups\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Clean Data\n",
        "\n",
        "Remove rows with NULL values in intercept_ground_code field.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create clean dataset by removing rows with NULL intercept_ground_code\n",
        "df_clean = df_current[df_current['intercept_ground_code'].notna()].copy()\n",
        "\n",
        "print(\"Clean Dataset Preparation:\")\n",
        "print(f\"  Original rows:    {len(df_current)}\")\n",
        "print(f\"  Rows with NULL intercept_ground_code: {len(df_current) - len(df_clean)}\")\n",
        "print(f\"  Clean rows:       {len(df_clean)}\")\n",
        "print(f\"  Rows to remove:   {len(df_current) - len(df_clean)}\")\n",
        "\n",
        "# Verify data integrity\n",
        "print(f\"\\nData Integrity Check:\")\n",
        "print(f\"  NULL intercept_ground_code in clean data: {df_clean['intercept_ground_code'].isna().sum()}\")\n",
        "print(f\"  All rows have ground code?: {df_clean['intercept_ground_code'].notna().all()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace Table with Clean Data\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT**: This will REPLACE the entire table with the clean dataset (no NULL intercept_ground_code rows).\n",
        "\n",
        "Review the summary above before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace table with clean data\n",
        "print(\"=\" * 60)\n",
        "print(\"REPLACING BIGQUERY TABLE WITH CLEAN DATA\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTable: {BQ_TABLE_ID}\")\n",
        "print(f\"Current rows: {len(df_current)}\")\n",
        "print(f\"New rows (clean): {len(df_clean)}\")\n",
        "print(f\"Rows removed: {len(df_current) - len(df_clean)}\")\n",
        "print(f\"Mode: WRITE_TRUNCATE (replace entire table)\")\n",
        "print(f\"\\nStarting replacement at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "\n",
        "# Configure job to replace existing table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  # Replace entire table\n",
        ")\n",
        "\n",
        "# Load clean dataframe to BigQuery\n",
        "load_job = bq_client.load_table_from_dataframe(\n",
        "    df_clean,\n",
        "    BQ_TABLE_ID,\n",
        "    job_config=job_config\n",
        ")\n",
        "\n",
        "# Wait for job to complete\n",
        "load_job.result()\n",
        "\n",
        "print(f\"\\n‚úì Replacement completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"  Rows written: {load_job.output_rows}\")\n",
        "print(f\"  Job ID: {load_job.job_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Fix\n",
        "\n",
        "Read back the table to verify NA rows have been removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read updated table\n",
        "print(\"Verifying fix...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "df_updated = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"\\n‚úì Verification query complete\")\n",
        "print(f\"  Rows in table: {len(df_updated)}\")\n",
        "print(f\"  Columns: {list(df_updated.columns)}\")\n",
        "\n",
        "# Check for NULL values\n",
        "print(f\"\\nNULL Value Check:\")\n",
        "null_counts_after = df_updated.isnull().sum()\n",
        "for col in df_updated.columns:\n",
        "    null_count = null_counts_after[col]\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col}: {null_count} NULLs (‚ö†Ô∏è UNEXPECTED)\")\n",
        "    else:\n",
        "        print(f\"  {col}: No NULLs ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify row counts\n",
        "expected_rows = len(df_clean)\n",
        "actual_rows = len(df_updated)\n",
        "\n",
        "print(\"\\nData integrity check:\")\n",
        "print(f\"  Expected rows:  {expected_rows}\")\n",
        "print(f\"  Actual rows:    {actual_rows}\")\n",
        "print(f\"  Rows removed:   {len(df_current) - actual_rows}\")\n",
        "\n",
        "if expected_rows == actual_rows:\n",
        "    print(f\"\\n‚úì Row count verified - table successfully cleaned\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† Row count mismatch!\")\n",
        "    print(f\"  Difference: {actual_rows - expected_rows}\")\n",
        "\n",
        "# Check if any NULL intercept_ground_code values remain\n",
        "null_ground_code_after = df_updated[df_updated['intercept_ground_code'].isna()]\n",
        "if len(null_ground_code_after) == 0:\n",
        "    print(f\"\\n‚úì SUCCESS: No NULL intercept_ground_code values found in updated table\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† WARNING: {len(null_ground_code_after)} NULL intercept_ground_code values still exist!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report\n",
        "\n",
        "Complete summary of the fix operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"FIX NA ROWS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\nüéØ Target:\")\n",
        "print(f\"  Table: {BQ_TABLE_ID}\")\n",
        "print(f\"  Project: {bq_client.project}\")\n",
        "\n",
        "print(f\"\\nüìä Data Changes:\")\n",
        "print(f\"  Original rows:  {len(df_current)}\")\n",
        "print(f\"  Cleaned rows:   {len(df_updated)}\")\n",
        "print(f\"  Rows removed:   {len(df_current) - len(df_updated)}\")\n",
        "\n",
        "if len(df_current) - len(df_updated) > 0:\n",
        "    print(f\"\\n  Removed rows had NULL values in intercept_ground_code\")\n",
        "\n",
        "print(f\"\\nüîÑ Operations Performed:\")\n",
        "print(f\"  ‚úì Backed up table to GCS\")\n",
        "print(f\"  ‚úì Removed rows with NULL intercept_ground_code\")\n",
        "print(f\"  ‚úì Replaced table with clean data\")\n",
        "print(f\"  ‚úì Verified data integrity\")\n",
        "\n",
        "if BACKUP_BUCKET:\n",
        "    print(f\"\\nüíæ Backup:\")\n",
        "    print(f\"  Location: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/\")\n",
        "    print(f\"  Status: ‚úì Created before fix\")\n",
        "\n",
        "# Final validation\n",
        "null_check = df_updated['intercept_ground_code'].isna().sum()\n",
        "if null_check == 0:\n",
        "    print(f\"\\n‚úÖ Fix completed successfully!\")\n",
        "    print(f\"   No NULL intercept_ground_code values remain in table\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è WARNING: {null_check} NULL values still exist\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollback Instructions (If Needed)\n",
        "\n",
        "If you need to rollback to the previous version, restore from the backup created at the beginning of this notebook.\n",
        "\n",
        "```python\n",
        "# To rollback, restore from backup:\n",
        "# backup_path = \"gs://BACKUP_BUCKET/BACKUP_PREFIX/fix_na_rows_TIMESTAMP/*.csv\"\n",
        "# df_backup = pd.read_csv(backup_path)\n",
        "# job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "# bq_client.load_table_from_dataframe(df_backup, BQ_TABLE_ID, job_config=job_config)\n",
        "```\n",
        "\n",
        "The backup location was printed in the backup cell above.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mpg-data-warehouse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
