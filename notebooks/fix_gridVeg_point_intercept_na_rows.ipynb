{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fix NA Rows in gridVeg Point Intercept Vegetation\n",
        "\n",
        "This notebook investigates and fixes NA/NULL rows in the BigQuery table `mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_point_intercept_vegetation`.\n",
        "\n",
        "**Operation**: Identify and remove rows with NULL values in critical fields\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values for gridVeg point intercepts\n",
        "BQ_TABLE_ID = config['gridveg_point_intercepts']['bigquery']['table_vegetation']\n",
        "BQ_PROJECT = config['gridveg_point_intercepts']['bigquery'].get('project')\n",
        "BACKUP_BUCKET = config['gridveg_point_intercepts']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gridveg_point_intercepts']['gcs'].get('backup_prefix', 'backups/gridveg_point_intercepts')\n",
        "\n",
        "# Verify required config values\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure gridveg_point_intercepts.bigquery.table_vegetation in config.yml\")\n",
        "\n",
        "print(\"✓ Configuration loaded successfully\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "\n",
        "print(f\"✓ BigQuery client initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Investigate Current Table State\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get table schema and basic info\n",
        "table = bq_client.get_table(BQ_TABLE_ID)\n",
        "\n",
        "print(\"Table Schema:\")\n",
        "for field in table.schema:\n",
        "    print(f\"  {field.name}: {field.field_type} (nullable: {field.mode != 'REQUIRED'})\")\n",
        "\n",
        "print(f\"\\nTotal rows in table: {table.num_rows}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query to get all data from the table\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "\n",
        "print(\"Loading current table data...\")\n",
        "df_current = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"✓ Data loaded: {len(df_current)} rows\")\n",
        "print(f\"  Columns: {list(df_current.columns)}\")\n",
        "\n",
        "# Display info\n",
        "df_current.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze NULL/NA Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for NULL values in each column\n",
        "print(\"NULL Value Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "null_counts = df_current.isnull().sum()\n",
        "null_percentages = (df_current.isnull().sum() / len(df_current) * 100)\n",
        "\n",
        "for col in df_current.columns:\n",
        "    null_count = null_counts[col]\n",
        "    null_pct = null_percentages[col]\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col:20s}: {null_count:5d} nulls ({null_pct:5.2f}%)\")\n",
        "    else:\n",
        "        print(f\"  {col:20s}: No nulls\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify rows with any NULL values\n",
        "rows_with_nulls = df_current[df_current.isnull().any(axis=1)]\n",
        "\n",
        "print(f\"Rows with at least one NULL value: {len(rows_with_nulls)}\")\n",
        "\n",
        "if len(rows_with_nulls) > 0:\n",
        "    print(f\"\\nBreakdown by column with NULL:\")\n",
        "    for col in df_current.columns:\n",
        "        null_in_col = df_current[df_current[col].isnull()]\n",
        "        if len(null_in_col) > 0:\n",
        "            print(f\"  {col}: {len(null_in_col)} rows\")\n",
        "    \n",
        "    print(f\"\\nSample of rows with NULL values:\")\n",
        "    display(rows_with_nulls.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA QUALITY ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTotal records in table: {len(df_current)}\")\n",
        "print(f\"Records with NULL values: {len(rows_with_nulls)} ({len(rows_with_nulls)/len(df_current)*100:.2f}%)\")\n",
        "print(f\"Clean records: {len(df_current) - len(rows_with_nulls)} ({(len(df_current) - len(rows_with_nulls))/len(df_current)*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nNULL values by column:\")\n",
        "for col in df_current.columns:\n",
        "    null_count = df_current[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col}: {null_count} ({null_count/len(df_current)*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backup Existing Table\n",
        "\n",
        "Before making any changes, create a backup of the existing table to GCS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backup existing table to GCS\n",
        "if BACKUP_BUCKET:\n",
        "    # Generate backup path with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = f\"gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/fix_na_rows_{timestamp}/*.csv\"\n",
        "    \n",
        "    print(f\"Creating backup of existing table...\")\n",
        "    print(f\"  Destination: {backup_path}\")\n",
        "    \n",
        "    # Export table to GCS\n",
        "    extract_job = bq_client.extract_table(\n",
        "        BQ_TABLE_ID,\n",
        "        backup_path,\n",
        "        location=\"US\"\n",
        "    )\n",
        "    \n",
        "    extract_job.result()  # Wait for job to complete\n",
        "    \n",
        "    print(f\"✓ Backup completed successfully\")\n",
        "    print(f\"  Files: {backup_path}\")\n",
        "else:\n",
        "    print(\"⚠ Backup bucket not configured in config.yml\")\n",
        "    print(\"  Set 'gridveg_point_intercepts.gcs.backup_bucket' to enable automatic backups\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Clean Data\n",
        "\n",
        "Remove rows with NULL values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create clean dataset by removing rows with any NULL values\n",
        "df_clean = df_current.dropna().copy()\n",
        "\n",
        "print(\"Clean Dataset Preparation:\")\n",
        "print(f\"  Original rows:    {len(df_current)}\")\n",
        "print(f\"  Rows with NULL:   {len(df_current) - len(df_clean)}\")\n",
        "print(f\"  Clean rows:       {len(df_clean)}\")\n",
        "print(f\"  Rows to remove:   {len(df_current) - len(df_clean)}\")\n",
        "\n",
        "# Verify data integrity\n",
        "print(f\"\\nData Integrity Check:\")\n",
        "print(f\"  Any NULL values in clean data?: {df_clean.isnull().any().any()}\")\n",
        "print(f\"  All rows complete?: {not df_clean.isnull().any().any()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace Table with Clean Data\n",
        "\n",
        "⚠️ **IMPORTANT**: This will REPLACE the entire table with the clean dataset (no NULL rows).\n",
        "\n",
        "Review the summary above before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace table with clean data\n",
        "print(\"=\" * 60)\n",
        "print(\"REPLACING BIGQUERY TABLE WITH CLEAN DATA\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTable: {BQ_TABLE_ID}\")\n",
        "print(f\"Current rows: {len(df_current)}\")\n",
        "print(f\"New rows (clean): {len(df_clean)}\")\n",
        "print(f\"Rows removed: {len(df_current) - len(df_clean)}\")\n",
        "print(f\"Mode: WRITE_TRUNCATE (replace entire table)\")\n",
        "print(f\"\\nStarting replacement at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "\n",
        "# Configure job to replace existing table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  # Replace entire table\n",
        ")\n",
        "\n",
        "# Load clean dataframe to BigQuery\n",
        "load_job = bq_client.load_table_from_dataframe(\n",
        "    df_clean,\n",
        "    BQ_TABLE_ID,\n",
        "    job_config=job_config\n",
        ")\n",
        "\n",
        "# Wait for job to complete\n",
        "load_job.result()\n",
        "\n",
        "print(f\"\\n✓ Replacement completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"  Rows written: {load_job.output_rows}\")\n",
        "print(f\"  Job ID: {load_job.job_id}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
