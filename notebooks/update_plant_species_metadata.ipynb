{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Update Plant Species Metadata in BigQuery\n",
        "\n",
        "This notebook updates plant species metadata in BigQuery from a CSV file stored in GCS.\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, google-cloud-storage, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values\n",
        "GCS_CSV_URL = config['gcs']['csv_url']\n",
        "BACKUP_BUCKET = config['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gcs'].get('backup_prefix', 'backups')\n",
        "BQ_TABLE_ID = config['bigquery']['table_id']\n",
        "BQ_PROJECT = config['bigquery'].get('project')\n",
        "\n",
        "# Verify required config values\n",
        "if not GCS_CSV_URL or GCS_CSV_URL.startswith('gs://your-'):\n",
        "    raise ValueError(\"Please configure gcs.csv_url in config.yml\")\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure bigquery.table_id in config.yml\")\n",
        "\n",
        "print(\"‚úì Configuration loaded successfully\")\n",
        "print(f\"  CSV URL: {GCS_CSV_URL[:50]}...\" if len(GCS_CSV_URL) > 50 else f\"  CSV URL: {GCS_CSV_URL}\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "storage_client = storage.Client(project=BQ_PROJECT) if BQ_PROJECT else storage.Client()\n",
        "\n",
        "print(f\"‚úì Clients initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV from GCS (new data)\n",
        "print(\"Reading CSV from GCS...\")\n",
        "# Use latin-1 encoding to handle special characters that aren't valid UTF-8\n",
        "df_new = pd.read_csv(GCS_CSV_URL, encoding='latin-1')\n",
        "\n",
        "print(f\"‚úì CSV loaded successfully:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {list(df_new.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_new.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform CSV Data\n",
        "\n",
        "Apply column transformations to match BigQuery schema:\n",
        "- Rename columns to follow warehouse naming conventions\n",
        "- Drop `zModificationTimestamp` column (not stored in warehouse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column mapping from CSV to BigQuery\n",
        "column_mapping = {\n",
        "    '__kp_PlantMetadata': 'key_plant_species',\n",
        "    '__kp_PlantCode': 'key_plant_code',\n",
        "    'NameScientific': 'plant_name_sci',\n",
        "    'NameSynonym': 'plant_name_syn',\n",
        "    'NameCommon': 'plant_name_common',\n",
        "    'NameFamily': 'plant_name_family',\n",
        "    'NativeStatus': 'plant_native_status',\n",
        "    'LifeCycle': 'plant_life_cycle',\n",
        "    'LifeForm': 'plant_life_form'\n",
        "    # zModificationTimestamp is dropped (not included in mapping)\n",
        "}\n",
        "\n",
        "print(\"Column mapping:\")\n",
        "for csv_col, bq_col in column_mapping.items():\n",
        "    print(f\"  {csv_col:25s} ‚Üí {bq_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify CSV columns match expected schema\n",
        "expected_csv_columns = set(column_mapping.keys()) | {'zModificationTimestamp'}\n",
        "actual_csv_columns = set(df_new.columns)\n",
        "\n",
        "if actual_csv_columns == expected_csv_columns:\n",
        "    print(\"‚úì CSV columns match expected schema\")\n",
        "else:\n",
        "    print(\"‚ö† CSV column differences detected:\")\n",
        "    if actual_csv_columns - expected_csv_columns:\n",
        "        print(f\"  Unexpected columns: {actual_csv_columns - expected_csv_columns}\")\n",
        "    if expected_csv_columns - actual_csv_columns:\n",
        "        print(f\"  Missing columns: {expected_csv_columns - actual_csv_columns}\")\n",
        "    \n",
        "print(f\"\\nCSV columns: {list(df_new.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply transformation: rename columns and drop zModificationTimestamp\n",
        "df_transformed = df_new.copy()\n",
        "\n",
        "# Select and rename columns in one step\n",
        "df_transformed = df_transformed[list(column_mapping.keys())].rename(columns=column_mapping)\n",
        "\n",
        "print(\"‚úì Transformation applied\")\n",
        "print(f\"  Original columns: {len(df_new.columns)}\")\n",
        "print(f\"  Transformed columns: {len(df_transformed.columns)}\")\n",
        "print(f\"  Dropped: zModificationTimestamp\")\n",
        "print(f\"\\nTransformed columns: {list(df_transformed.columns)}\")\n",
        "print(f\"\\nTransformed data preview:\")\n",
        "df_transformed.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic statistics about the transformed data\n",
        "print(\"Transformed Data Info:\")\n",
        "df_transformed.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Existing BigQuery Table\n",
        "\n",
        "Load the current data from BigQuery to compare with the new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read existing data from BigQuery\n",
        "print(f\"Reading existing data from {BQ_TABLE_ID}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "\n",
        "try:\n",
        "    df_existing = bq_client.query(query).to_dataframe()\n",
        "    print(f\"‚úì Existing table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing)}\")\n",
        "    print(f\"  Columns: {list(df_existing.columns)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö† Error reading table: {e}\")\n",
        "    print(\"  This may be expected if the table doesn't exist yet.\")\n",
        "    df_existing = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display existing data (if available)\n",
        "if df_existing is not None:\n",
        "    print(\"Existing data sample:\")\n",
        "    display(df_existing.head())\n",
        "    print(\"\\nExisting Data Info:\")\n",
        "    df_existing.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Differences\n",
        "\n",
        "Compare the new CSV data with the existing BigQuery table to identify changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare datasets (using transformed data)\n",
        "if df_existing is not None:\n",
        "    print(\"=== Comparison Summary ===\\n\")\n",
        "    \n",
        "    # Row count comparison\n",
        "    print(f\"Row count:\")\n",
        "    print(f\"  Existing: {len(df_existing)}\")\n",
        "    print(f\"  New:      {len(df_transformed)}\")\n",
        "    print(f\"  Diff:     {len(df_transformed) - len(df_existing):+d}\\n\")\n",
        "    \n",
        "    # Column comparison\n",
        "    existing_cols = set(df_existing.columns)\n",
        "    new_cols = set(df_transformed.columns)\n",
        "    \n",
        "    if existing_cols == new_cols:\n",
        "        print(f\"‚úì Columns match ({len(new_cols)} columns)\")\n",
        "    else:\n",
        "        print(\"‚ö† Column differences detected:\")\n",
        "        if new_cols - existing_cols:\n",
        "            print(f\"  New columns: {new_cols - existing_cols}\")\n",
        "        if existing_cols - new_cols:\n",
        "            print(f\"  Removed columns: {existing_cols - new_cols}\")\n",
        "    \n",
        "    print(f\"\\nColumns: {list(df_transformed.columns)}\")\n",
        "    \n",
        "    # Data type comparison\n",
        "    if existing_cols == new_cols:\n",
        "        print(f\"\\nData types comparison:\")\n",
        "        for col in df_transformed.columns:\n",
        "            existing_type = str(df_existing[col].dtype)\n",
        "            new_type = str(df_transformed[col].dtype)\n",
        "            match_symbol = \"‚úì\" if existing_type == new_type else \"‚ö†\"\n",
        "            print(f\"  {match_symbol} {col:25s} existing: {existing_type:10s} ‚Üí new: {new_type:10s}\")\n",
        "else:\n",
        "    print(\"No existing data to compare - this will be a new table creation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify new and removed records\n",
        "if df_existing is not None and len(df_transformed) != len(df_existing):\n",
        "    # Find records in new data that aren't in existing (based on key_plant_code)\n",
        "    existing_keys = set(df_existing['key_plant_code'])\n",
        "    new_keys = set(df_transformed['key_plant_code'])\n",
        "    \n",
        "    added_keys = new_keys - existing_keys\n",
        "    removed_keys = existing_keys - new_keys\n",
        "    \n",
        "    if added_keys:\n",
        "        print(f\"‚úì New records to add ({len(added_keys)}):\")\n",
        "        new_records = df_transformed[df_transformed['key_plant_code'].isin(added_keys)]\n",
        "        display(new_records[['key_plant_code', 'plant_name_sci', 'plant_name_common']])\n",
        "    \n",
        "    if removed_keys:\n",
        "        print(f\"\\n‚ö† Records to remove ({len(removed_keys)}):\")\n",
        "        removed_records = df_existing[df_existing['key_plant_code'].isin(removed_keys)]\n",
        "        display(removed_records[['key_plant_code', 'plant_name_sci', 'plant_name_common']])\n",
        "    \n",
        "    if not added_keys and not removed_keys:\n",
        "        print(\"No records added or removed - only updates to existing records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backup Existing Table\n",
        "\n",
        "Before making any changes, create a backup of the existing table to GCS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backup existing table to GCS\n",
        "from datetime import datetime\n",
        "\n",
        "if df_existing is not None and BACKUP_BUCKET:\n",
        "    # Generate backup path with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = f\"gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/{timestamp}/*.csv\"\n",
        "    \n",
        "    print(f\"Creating backup of existing table...\")\n",
        "    print(f\"  Destination: {backup_path}\")\n",
        "    \n",
        "    # Export table to GCS\n",
        "    extract_job = bq_client.extract_table(\n",
        "        BQ_TABLE_ID,\n",
        "        backup_path,\n",
        "        location=\"US\"\n",
        "    )\n",
        "    \n",
        "    extract_job.result()  # Wait for job to complete\n",
        "    \n",
        "    print(f\"‚úì Backup completed successfully\")\n",
        "    print(f\"  Files: {backup_path}\")\n",
        "elif df_existing is None:\n",
        "    print(\"‚ö† No existing table to backup (table doesn't exist yet)\")\n",
        "elif not BACKUP_BUCKET:\n",
        "    print(\"‚ö† Backup bucket not configured in config.yml\")\n",
        "    print(\"  Set 'gcs.backup_bucket' to enable automatic backups\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Update BigQuery Table\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT**: This will replace the entire table with the new data.\n",
        "\n",
        "Review the comparison above before proceeding. The backup has been created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write transformed data to BigQuery\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"UPDATING BIGQUERY TABLE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTable: {BQ_TABLE_ID}\")\n",
        "print(f\"Rows to write: {len(df_transformed)}\")\n",
        "print(f\"Mode: WRITE_TRUNCATE (replace entire table)\")\n",
        "print(f\"\\nStarting update at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "\n",
        "# Configure job to replace entire table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\",  # Replace entire table\n",
        "    schema_update_options=[\n",
        "        bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,\n",
        "        bigquery.SchemaUpdateOption.ALLOW_FIELD_RELAXATION\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load dataframe to BigQuery\n",
        "load_job = bq_client.load_table_from_dataframe(\n",
        "    df_transformed,\n",
        "    BQ_TABLE_ID,\n",
        "    job_config=job_config\n",
        ")\n",
        "\n",
        "# Wait for job to complete\n",
        "load_job.result()\n",
        "\n",
        "print(f\"\\n‚úì Update completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"  Rows written: {load_job.output_rows}\")\n",
        "print(f\"  Job ID: {load_job.job_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Update\n",
        "\n",
        "Read back the table to verify the update was successful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read updated table\n",
        "print(\"Verifying update...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "df_updated = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"\\n‚úì Verification complete\")\n",
        "print(f\"  Rows in table: {len(df_updated)}\")\n",
        "print(f\"  Columns: {list(df_updated.columns)}\")\n",
        "print(f\"\\nUpdated table preview:\")\n",
        "df_updated.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify row counts match\n",
        "print(\"Data integrity check:\")\n",
        "print(f\"  Rows written:  {len(df_transformed)}\")\n",
        "print(f\"  Rows in table: {len(df_updated)}\")\n",
        "\n",
        "if len(df_transformed) == len(df_updated):\n",
        "    print(f\"\\n‚úì Row count verified - all {len(df_updated)} rows successfully written\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† Row count mismatch!\")\n",
        "    print(f\"  Expected: {len(df_transformed)}\")\n",
        "    print(f\"  Actual:   {len(df_updated)}\")\n",
        "    print(f\"  Difference: {len(df_updated) - len(df_transformed)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report\n",
        "\n",
        "Complete summary of the update operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PLANT SPECIES METADATA UPDATE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\nüìÇ Source:\")\n",
        "print(f\"  CSV: {GCS_CSV_URL.split('/')[-1]}\")\n",
        "print(f\"  Location: {'/'.join(GCS_CSV_URL.split('/')[:-1])}\")\n",
        "\n",
        "print(f\"\\nüéØ Target:\")\n",
        "print(f\"  Table: {BQ_TABLE_ID}\")\n",
        "print(f\"  Project: {bq_client.project}\")\n",
        "\n",
        "print(f\"\\nüìä Data Changes:\")\n",
        "if df_existing is not None:\n",
        "    print(f\"  Previous rows: {len(df_existing)}\")\n",
        "    print(f\"  New rows:      {len(df_updated)}\")\n",
        "    print(f\"  Net change:    {len(df_updated) - len(df_existing):+d}\")\n",
        "else:\n",
        "    print(f\"  New table created with {len(df_updated)} rows\")\n",
        "\n",
        "print(f\"\\nüîÑ Transformations Applied:\")\n",
        "print(f\"  ‚úì Renamed {len(column_mapping)} columns to warehouse conventions\")\n",
        "print(f\"  ‚úì Dropped zModificationTimestamp column\")\n",
        "\n",
        "if BACKUP_BUCKET and df_existing is not None:\n",
        "    print(f\"\\nüíæ Backup:\")\n",
        "    print(f\"  Location: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/\")\n",
        "    print(f\"  Status: ‚úì Created before update\")\n",
        "\n",
        "print(f\"\\n‚úÖ Update completed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollback Instructions (If Needed)\n",
        "\n",
        "If you need to rollback to the previous version, use the backup created at the beginning of this notebook.\n",
        "\n",
        "```python\n",
        "# To rollback, load from the backup:\n",
        "# backup_path = \"gs://mpg-data-warehouse/backups/vegetation_species_metadata/YYYYMMDD_HHMMSS/*.csv\"\n",
        "# df_backup = pd.read_csv(backup_path)\n",
        "# bq_client.load_table_from_dataframe(df_backup, BQ_TABLE_ID, job_config=job_config)\n",
        "```\n",
        "\n",
        "The backup location was printed in the backup cell above.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gcloud",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
