{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Append GridVeg Species Richness to BigQuery\n",
        "\n",
        "This notebook appends gridVeg species richness data from CSV in GCS to the BigQuery table.\n",
        "\n",
        "## Features\n",
        "- ‚úÖ Reads CSV from Google Cloud Storage\n",
        "- ‚úÖ Creates backup of existing BigQuery table before appending\n",
        "- ‚úÖ Appends new data to existing table (WRITE_APPEND mode)\n",
        "- ‚úÖ Validates data integrity after append\n",
        "- ‚úÖ Provides detailed summary report\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: `config.yml` with `csv_append` section configured\n",
        "- Required packages: google-cloud-bigquery, google-cloud-storage, pandas, pyyaml\n",
        "\n",
        "## Configuration\n",
        "Update `config.yml` with your specific values in the `csv_append` section:\n",
        "```yaml\n",
        "csv_append:\n",
        "  gcs:\n",
        "    csv_url: \"gs://your-bucket/path/to/data.csv\"\n",
        "    backup_bucket: \"your-bucket\"\n",
        "    backup_prefix: \"backups/csv_append\"\n",
        "  bigquery:\n",
        "    table_id: \"your-project.your_dataset.your_table\"\n",
        "    project: \"your-project-id\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n",
            "Libraries imported successfully\n",
            "Timestamp: 2025-11-06 11:40:05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/esamsoe/miniforge3-new/envs/mpg-data-warehouse/lib/python3.9/site-packages/google/api_core/_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.23) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/Users/esamsoe/miniforge3-new/envs/mpg-data-warehouse/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n",
        "\n",
        "Load settings from `config.yml` including:\n",
        "- CSV source URL in GCS\n",
        "- BigQuery table information\n",
        "- Backup location settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Configuration loaded successfully\n",
            "  Config section: gridveg_species_richness_append\n",
            "  CSV URL: gs://mpg-data-warehouse/gridVeg/src/2025/gridVeg_s...\n",
            "  Table ID: mpg-data-warehouse.vegetation_gridVeg_summaries.gridVeg_species_richness\n",
            "  Backup: gs://mpg-data-warehouse-backups/backups/vegetation_gridVeg_summaries/gridVeg_species_richness\n"
          ]
        }
      ],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Configuration section for GridVeg Species Richness\n",
        "CONFIG_SECTION = 'gridveg_species_richness_append'\n",
        "\n",
        "# Extract configuration values\n",
        "GCS_CSV_URL = config[CONFIG_SECTION]['gcs']['csv_url']\n",
        "BACKUP_BUCKET = config[CONFIG_SECTION]['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config[CONFIG_SECTION]['gcs'].get('backup_prefix', 'backups')\n",
        "BQ_TABLE_ID = config[CONFIG_SECTION]['bigquery']['table_id']\n",
        "BQ_PROJECT = config[CONFIG_SECTION]['bigquery'].get('project')\n",
        "\n",
        "# Verify required config values\n",
        "if not GCS_CSV_URL or GCS_CSV_URL.startswith('gs://your-'):\n",
        "    raise ValueError(\"Please configure csv_append.gcs.csv_url in config.yml\")\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure csv_append.bigquery.table_id in config.yml\")\n",
        "\n",
        "print(\"‚úì Configuration loaded successfully\")\n",
        "print(f\"  Config section: {CONFIG_SECTION}\")\n",
        "print(f\"  CSV URL: {GCS_CSV_URL[:50]}...\" if len(GCS_CSV_URL) > 50 else f\"  CSV URL: {GCS_CSV_URL}\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Clients initialized\n",
            "  Project: mpg-data-warehouse\n"
          ]
        }
      ],
      "source": [
        "# Initialize Google Cloud clients\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "storage_client = storage.Client(project=BQ_PROJECT) if BQ_PROJECT else storage.Client()\n",
        "\n",
        "print(f\"‚úì Clients initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Existing BigQuery Table\n",
        "\n",
        "Load the current table to:\n",
        "- Verify it exists\n",
        "- Get row count before append\n",
        "- Prepare for backup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading existing data from mpg-data-warehouse.vegetation_gridVeg_summaries.gridVeg_species_richness...\n",
            "‚úì Existing table loaded:\n",
            "  Rows: 38056\n",
            "  Columns: ['survey_ID', 'grid_point', 'year', 'key_plant_species', 'detection_type']\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>year</th>\n",
              "      <th>key_plant_species</th>\n",
              "      <th>detection_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>69</td>\n",
              "      <td>329</td>\n",
              "      <td>2011</td>\n",
              "      <td>435</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>69</td>\n",
              "      <td>329</td>\n",
              "      <td>2011</td>\n",
              "      <td>82</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>69</td>\n",
              "      <td>329</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>69</td>\n",
              "      <td>329</td>\n",
              "      <td>2011</td>\n",
              "      <td>496</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>329</td>\n",
              "      <td>2011</td>\n",
              "      <td>497</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  survey_ID  grid_point  year  key_plant_species   detection_type\n",
              "0        69         329  2011                435  point_intercept\n",
              "1        69         329  2011                 82  point_intercept\n",
              "2        69         329  2011                 12  point_intercept\n",
              "3        69         329  2011                496  point_intercept\n",
              "4        69         329  2011                497  point_intercept"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Read existing data from BigQuery\n",
        "print(f\"Reading existing data from {BQ_TABLE_ID}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "\n",
        "try:\n",
        "    df_existing = bq_client.query(query).to_dataframe()\n",
        "    print(f\"‚úì Existing table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing)}\")\n",
        "    print(f\"  Columns: {list(df_existing.columns)}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(df_existing.head())\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Error reading table: {e}\")\n",
        "    print(\"  The table must exist before appending data.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backup Existing Table to GCS\n",
        "\n",
        "‚ö†Ô∏è **CRITICAL STEP**: Create a backup of the existing table before appending new data.\n",
        "\n",
        "This backup can be used to restore the table if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating backup of existing table...\n",
            "  Source table: mpg-data-warehouse.vegetation_gridVeg_summaries.gridVeg_species_richness\n",
            "  Destination: gs://mpg-data-warehouse-backups/backups/vegetation_gridVeg_summaries/gridVeg_species_richness/20251106_114025/*.csv\n",
            "  Rows to backup: 38056\n",
            "\n",
            "‚úì Backup completed successfully\n",
            "  Job ID: dd027685-778d-44fc-994a-a829ad115c11\n",
            "  Backup location: gs://mpg-data-warehouse-backups/backups/vegetation_gridVeg_summaries/gridVeg_species_richness/20251106_114025/*.csv\n"
          ]
        }
      ],
      "source": [
        "# Backup existing table to GCS\n",
        "if BACKUP_BUCKET:\n",
        "    # Generate backup path with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = f\"gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/{timestamp}/*.csv\"\n",
        "    \n",
        "    print(f\"Creating backup of existing table...\")\n",
        "    print(f\"  Source table: {BQ_TABLE_ID}\")\n",
        "    print(f\"  Destination: {backup_path}\")\n",
        "    print(f\"  Rows to backup: {len(df_existing)}\")\n",
        "    \n",
        "    # Export table to GCS\n",
        "    extract_job = bq_client.extract_table(\n",
        "        BQ_TABLE_ID,\n",
        "        backup_path,\n",
        "        location=\"US\"\n",
        "    )\n",
        "    \n",
        "    # Wait for job to complete\n",
        "    extract_job.result()\n",
        "    \n",
        "    print(f\"\\n‚úì Backup completed successfully\")\n",
        "    print(f\"  Job ID: {extract_job.job_id}\")\n",
        "    print(f\"  Backup location: {backup_path}\")\n",
        "    \n",
        "    # Store backup info for later reference\n",
        "    BACKUP_LOCATION = backup_path\n",
        "    BACKUP_TIMESTAMP = timestamp\n",
        "else:\n",
        "    print(\"‚ö† Backup bucket not configured in config.yml\")\n",
        "    print(\"  Set 'csv_append.gcs.backup_bucket' to enable automatic backups\")\n",
        "    print(\"  Proceeding without backup...\")\n",
        "    BACKUP_LOCATION = None\n",
        "    BACKUP_TIMESTAMP = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read CSV from GCS\n",
        "\n",
        "Load the new data to be appended to the BigQuery table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading CSV from GCS...\n",
            "  Source: gs://mpg-data-warehouse/gridVeg/src/2025/gridVeg_species_richness_WRANGLE-251104.csv\n",
            "\n",
            "‚úì CSV loaded successfully:\n",
            "  Rows: 2597\n",
            "  Columns: ['survey_ID', 'grid_point', 'year', 'key_plant_species', 'detection_type']\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>year</th>\n",
              "      <th>key_plant_species</th>\n",
              "      <th>detection_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>529</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>232</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>320</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>265</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>80</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              survey_ID  grid_point  year  key_plant_species  \\\n",
              "0  27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD           3  2023                529   \n",
              "1  27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD           3  2023                232   \n",
              "2  27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD           3  2023                320   \n",
              "3  27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD           3  2023                265   \n",
              "4  27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD           3  2023                 80   \n",
              "\n",
              "    detection_type  \n",
              "0  point_intercept  \n",
              "1  point_intercept  \n",
              "2  point_intercept  \n",
              "3  point_intercept  \n",
              "4  point_intercept  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read CSV from GCS\n",
        "print(f\"Reading CSV from GCS...\")\n",
        "print(f\"  Source: {GCS_CSV_URL}\")\n",
        "\n",
        "try:\n",
        "    # Try UTF-8 first, fallback to latin-1 if needed\n",
        "    df_new = pd.read_csv(GCS_CSV_URL, encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"  Note: Using latin-1 encoding to handle special characters\")\n",
        "    df_new = pd.read_csv(GCS_CSV_URL, encoding='latin-1')\n",
        "\n",
        "print(f\"\\n‚úì CSV loaded successfully:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {list(df_new.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_new.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Schema Compatibility\n",
        "\n",
        "Verify that the CSV columns match the existing BigQuery table schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Schema Validation ===\n",
            "\n",
            "‚úì Column names match (5 columns)\n",
            "\n",
            "Columns: ['survey_ID', 'grid_point', 'year', 'key_plant_species', 'detection_type']\n",
            "\n",
            "Data type comparison:\n",
            "  ‚úì survey_ID                      existing: object     ‚Üí new: object    \n",
            "  ‚ö† grid_point                     existing: Int64      ‚Üí new: int64     \n",
            "  ‚ö† year                           existing: Int64      ‚Üí new: int64     \n",
            "  ‚ö† key_plant_species              existing: Int64      ‚Üí new: int64     \n",
            "  ‚úì detection_type                 existing: object     ‚Üí new: object    \n",
            "\n",
            "‚úì Schema validation complete\n"
          ]
        }
      ],
      "source": [
        "# Validate schema compatibility\n",
        "print(\"=== Schema Validation ===\\n\")\n",
        "\n",
        "# Check column names\n",
        "existing_cols = set(df_existing.columns)\n",
        "new_cols = set(df_new.columns)\n",
        "\n",
        "if existing_cols == new_cols:\n",
        "    print(f\"‚úì Column names match ({len(new_cols)} columns)\")\n",
        "else:\n",
        "    print(\"‚ö† Column differences detected:\")\n",
        "    if new_cols - existing_cols:\n",
        "        print(f\"  Extra columns in CSV: {new_cols - existing_cols}\")\n",
        "    if existing_cols - new_cols:\n",
        "        print(f\"  Missing columns in CSV: {existing_cols - new_cols}\")\n",
        "    \n",
        "    user_input = input(\"\\nContinue anyway? (yes/no): \")\n",
        "    if user_input.lower() != 'yes':\n",
        "        raise ValueError(\"Schema mismatch - aborting append operation\")\n",
        "\n",
        "print(f\"\\nColumns: {list(df_new.columns)}\")\n",
        "\n",
        "# Check data types\n",
        "print(f\"\\nData type comparison:\")\n",
        "for col in df_new.columns:\n",
        "    if col in df_existing.columns:\n",
        "        existing_type = str(df_existing[col].dtype)\n",
        "        new_type = str(df_new[col].dtype)\n",
        "        match_symbol = \"‚úì\" if existing_type == new_type else \"‚ö†\"\n",
        "        print(f\"  {match_symbol} {col:30s} existing: {existing_type:10s} ‚Üí new: {new_type:10s}\")\n",
        "\n",
        "print(f\"\\n‚úì Schema validation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Data Types\n",
        "\n",
        "Convert data types in the new data to match the existing table schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting data types to match existing schema...\n",
            "\n",
            "‚úì Data type conversions complete\n",
            "\n",
            "Updated data types:\n",
            "  ‚úì survey_ID                      existing: object     ‚Üí new: object    \n",
            "  ‚ö† grid_point                     existing: Int64      ‚Üí new: int64     \n",
            "  ‚ö† year                           existing: Int64      ‚Üí new: int64     \n",
            "  ‚ö† key_plant_species              existing: Int64      ‚Üí new: int64     \n",
            "  ‚úì detection_type                 existing: object     ‚Üí new: object    \n"
          ]
        }
      ],
      "source": [
        "# Convert data types to match existing table\n",
        "print(\"Converting data types to match existing schema...\")\n",
        "\n",
        "# Convert survey_sequence to string to match existing table\n",
        "if 'survey_sequence' in df_new.columns:\n",
        "    df_new['survey_sequence'] = df_new['survey_sequence'].astype(str)\n",
        "    print(f\"  ‚úì survey_sequence: int64 ‚Üí object (string)\")\n",
        "\n",
        "print(\"\\n‚úì Data type conversions complete\")\n",
        "print(\"\\nUpdated data types:\")\n",
        "for col in df_new.columns:\n",
        "    if col in df_existing.columns:\n",
        "        existing_type = str(df_existing[col].dtype)\n",
        "        new_type = str(df_new[col].dtype)\n",
        "        match_symbol = \"‚úì\" if existing_type == new_type else \"‚ö†\"\n",
        "        print(f\"  {match_symbol} {col:30s} existing: {existing_type:10s} ‚Üí new: {new_type:10s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preview Data Comparison\n",
        "\n",
        "Compare existing and new data before appending.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Summary ===\n",
            "\n",
            "Existing table:\n",
            "  Rows: 38056\n",
            "  Columns: 5\n",
            "\n",
            "New data to append:\n",
            "  Rows: 2597\n",
            "  Columns: 5\n",
            "\n",
            "After append:\n",
            "  Expected total rows: 40653\n",
            "\n",
            "--- Existing Data Sample ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>year</th>\n",
              "      <th>key_plant_species</th>\n",
              "      <th>detection_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>69</td>\n",
              "      <td>329</td>\n",
              "      <td>2011</td>\n",
              "      <td>435</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>69</td>\n",
              "      <td>329</td>\n",
              "      <td>2011</td>\n",
              "      <td>82</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>69</td>\n",
              "      <td>329</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  survey_ID  grid_point  year  key_plant_species   detection_type\n",
              "0        69         329  2011                435  point_intercept\n",
              "1        69         329  2011                 82  point_intercept\n",
              "2        69         329  2011                 12  point_intercept"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- New Data Sample ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>year</th>\n",
              "      <th>key_plant_species</th>\n",
              "      <th>detection_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>529</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>232</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>320</td>\n",
              "      <td>point_intercept</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              survey_ID  grid_point  year  key_plant_species  \\\n",
              "0  27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD           3  2023                529   \n",
              "1  27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD           3  2023                232   \n",
              "2  27869B01-61AE-4DB3-A2AC-AD4D8C9A7ECD           3  2023                320   \n",
              "\n",
              "    detection_type  \n",
              "0  point_intercept  \n",
              "1  point_intercept  \n",
              "2  point_intercept  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display data summary\n",
        "print(\"=== Data Summary ===\\n\")\n",
        "print(f\"Existing table:\")\n",
        "print(f\"  Rows: {len(df_existing)}\")\n",
        "print(f\"  Columns: {len(df_existing.columns)}\")\n",
        "\n",
        "print(f\"\\nNew data to append:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {len(df_new.columns)}\")\n",
        "\n",
        "print(f\"\\nAfter append:\")\n",
        "print(f\"  Expected total rows: {len(df_existing) + len(df_new)}\")\n",
        "\n",
        "print(\"\\n--- Existing Data Sample ---\")\n",
        "display(df_existing.head(3))\n",
        "\n",
        "print(\"\\n--- New Data Sample ---\")\n",
        "display(df_new.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Append Data to BigQuery Table\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT**: This will append new rows to the existing table.\n",
        "\n",
        "The backup has been created. Review the data above before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "APPENDING DATA TO BIGQUERY TABLE\n",
            "============================================================\n",
            "\n",
            "Table: mpg-data-warehouse.vegetation_gridVeg_summaries.gridVeg_species_richness\n",
            "Rows to append: 2597\n",
            "Current rows: 38056\n",
            "Mode: WRITE_APPEND (add to existing table)\n",
            "\n",
            "Starting append at 2025-11-06 11:41:03...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/esamsoe/miniforge3-new/envs/mpg-data-warehouse/lib/python3.9/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úì Append completed at 2025-11-06 11:41:06\n",
            "  Rows appended: 2597\n",
            "  Job ID: d16b927f-2408-462d-947e-cd3e88fc5b55\n"
          ]
        }
      ],
      "source": [
        "# Append data to BigQuery table\n",
        "print(\"=\" * 60)\n",
        "print(\"APPENDING DATA TO BIGQUERY TABLE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTable: {BQ_TABLE_ID}\")\n",
        "print(f\"Rows to append: {len(df_new)}\")\n",
        "print(f\"Current rows: {len(df_existing)}\")\n",
        "print(f\"Mode: WRITE_APPEND (add to existing table)\")\n",
        "print(f\"\\nStarting append at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "\n",
        "# Configure job to append to existing table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_APPEND\"  # Append to existing table\n",
        ")\n",
        "\n",
        "# Load dataframe to BigQuery\n",
        "load_job = bq_client.load_table_from_dataframe(\n",
        "    df_new,\n",
        "    BQ_TABLE_ID,\n",
        "    job_config=job_config\n",
        ")\n",
        "\n",
        "# Wait for job to complete\n",
        "load_job.result()\n",
        "\n",
        "print(f\"\\n‚úì Append completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"  Rows appended: {load_job.output_rows}\")\n",
        "print(f\"  Job ID: {load_job.job_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Append Operation\n",
        "\n",
        "Read back the table to verify the append was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying append operation...\n",
            "\n",
            "‚úì Verification complete\n",
            "  Rows in table: 40653\n",
            "  Columns: ['survey_ID', 'grid_point', 'year', 'key_plant_species', 'detection_type']\n",
            "\n",
            "Last few rows of updated table (should include new data):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>year</th>\n",
              "      <th>key_plant_species</th>\n",
              "      <th>detection_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40648</th>\n",
              "      <td>FFFC121E-C275-4271-B8C6-F8AA7503225C</td>\n",
              "      <td>240</td>\n",
              "      <td>2016</td>\n",
              "      <td>530</td>\n",
              "      <td>supplemental_obs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40649</th>\n",
              "      <td>FFFC121E-C275-4271-B8C6-F8AA7503225C</td>\n",
              "      <td>240</td>\n",
              "      <td>2016</td>\n",
              "      <td>545</td>\n",
              "      <td>supplemental_obs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40650</th>\n",
              "      <td>FFFC121E-C275-4271-B8C6-F8AA7503225C</td>\n",
              "      <td>240</td>\n",
              "      <td>2016</td>\n",
              "      <td>561</td>\n",
              "      <td>supplemental_obs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40651</th>\n",
              "      <td>FFFC121E-C275-4271-B8C6-F8AA7503225C</td>\n",
              "      <td>240</td>\n",
              "      <td>2016</td>\n",
              "      <td>520</td>\n",
              "      <td>supplemental_obs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40652</th>\n",
              "      <td>FFFC121E-C275-4271-B8C6-F8AA7503225C</td>\n",
              "      <td>240</td>\n",
              "      <td>2016</td>\n",
              "      <td>522</td>\n",
              "      <td>supplemental_obs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  survey_ID  grid_point  year  \\\n",
              "40648  FFFC121E-C275-4271-B8C6-F8AA7503225C         240  2016   \n",
              "40649  FFFC121E-C275-4271-B8C6-F8AA7503225C         240  2016   \n",
              "40650  FFFC121E-C275-4271-B8C6-F8AA7503225C         240  2016   \n",
              "40651  FFFC121E-C275-4271-B8C6-F8AA7503225C         240  2016   \n",
              "40652  FFFC121E-C275-4271-B8C6-F8AA7503225C         240  2016   \n",
              "\n",
              "       key_plant_species    detection_type  \n",
              "40648                530  supplemental_obs  \n",
              "40649                545  supplemental_obs  \n",
              "40650                561  supplemental_obs  \n",
              "40651                520  supplemental_obs  \n",
              "40652                522  supplemental_obs  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read updated table\n",
        "print(\"Verifying append operation...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "df_updated = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"\\n‚úì Verification complete\")\n",
        "print(f\"  Rows in table: {len(df_updated)}\")\n",
        "print(f\"  Columns: {list(df_updated.columns)}\")\n",
        "print(f\"\\nLast few rows of updated table (should include new data):\")\n",
        "df_updated.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data integrity check:\n",
            "  Rows before append:  38056\n",
            "  Rows appended:       2597\n",
            "  Expected total:      40653\n",
            "  Actual rows in table: 40653\n",
            "\n",
            "‚úì Row count verified - all 2597 rows successfully appended\n"
          ]
        }
      ],
      "source": [
        "# Verify row counts\n",
        "print(\"Data integrity check:\")\n",
        "print(f\"  Rows before append:  {len(df_existing)}\")\n",
        "print(f\"  Rows appended:       {len(df_new)}\")\n",
        "print(f\"  Expected total:      {len(df_existing) + len(df_new)}\")\n",
        "print(f\"  Actual rows in table: {len(df_updated)}\")\n",
        "\n",
        "if len(df_updated) == len(df_existing) + len(df_new):\n",
        "    print(f\"\\n‚úì Row count verified - all {len(df_new)} rows successfully appended\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† Row count mismatch!\")\n",
        "    print(f\"  Expected: {len(df_existing) + len(df_new)}\")\n",
        "    print(f\"  Actual:   {len(df_updated)}\")\n",
        "    print(f\"  Difference: {len(df_updated) - (len(df_existing) + len(df_new))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report\n",
        "\n",
        "Complete summary of the append operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CSV APPEND TO BIGQUERY - SUMMARY REPORT\n",
            "============================================================\n",
            "\n",
            "üìÖ Timestamp: 2025-11-06 11:41:21\n",
            "\n",
            "üìÇ Source:\n",
            "  CSV: gridVeg_species_richness_WRANGLE-251104.csv\n",
            "  Location: gs://mpg-data-warehouse/gridVeg/src/2025\n",
            "\n",
            "üéØ Target:\n",
            "  Table: mpg-data-warehouse.vegetation_gridVeg_summaries.gridVeg_species_richness\n",
            "  Project: mpg-data-warehouse\n",
            "\n",
            "üìä Data Changes:\n",
            "  Rows before:  38056\n",
            "  Rows added:   2597\n",
            "  Rows after:   40653\n",
            "  Net change:   +2597\n",
            "\n",
            "üíæ Backup:\n",
            "  Location: gs://mpg-data-warehouse-backups/backups/vegetation_gridVeg_summaries/gridVeg_species_richness/20251106_114025/*.csv\n",
            "  Timestamp: 20251106_114025\n",
            "  Status: ‚úì Created before append\n",
            "\n",
            "‚úÖ Append completed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"CSV APPEND TO BIGQUERY - SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\nüìÇ Source:\")\n",
        "print(f\"  CSV: {GCS_CSV_URL.split('/')[-1]}\")\n",
        "print(f\"  Location: {'/'.join(GCS_CSV_URL.split('/')[:-1])}\")\n",
        "\n",
        "print(f\"\\nüéØ Target:\")\n",
        "print(f\"  Table: {BQ_TABLE_ID}\")\n",
        "print(f\"  Project: {bq_client.project}\")\n",
        "\n",
        "print(f\"\\nüìä Data Changes:\")\n",
        "print(f\"  Rows before:  {len(df_existing)}\")\n",
        "print(f\"  Rows added:   {len(df_new)}\")\n",
        "print(f\"  Rows after:   {len(df_updated)}\")\n",
        "print(f\"  Net change:   +{len(df_updated) - len(df_existing)}\")\n",
        "\n",
        "if BACKUP_LOCATION:\n",
        "    print(f\"\\nüíæ Backup:\")\n",
        "    print(f\"  Location: {BACKUP_LOCATION}\")\n",
        "    print(f\"  Timestamp: {BACKUP_TIMESTAMP}\")\n",
        "    print(f\"  Status: ‚úì Created before append\")\n",
        "else:\n",
        "    print(f\"\\nüíæ Backup:\")\n",
        "    print(f\"  Status: ‚ö† No backup created\")\n",
        "\n",
        "print(f\"\\n‚úÖ Append completed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollback Instructions (If Needed)\n",
        "\n",
        "If you need to rollback to the previous version, restore from the backup created at the beginning of this notebook.\n",
        "\n",
        "### Option 1: Restore from BigQuery backup\n",
        "\n",
        "```python\n",
        "# Replace table with backup data\n",
        "backup_path = \"gs://BUCKET/PREFIX/TIMESTAMP/*.csv\"\n",
        "df_backup = pd.read_csv(backup_path)\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  # Replace entire table\n",
        ")\n",
        "\n",
        "load_job = bq_client.load_table_from_dataframe(\n",
        "    df_backup,\n",
        "    BQ_TABLE_ID,\n",
        "    job_config=job_config\n",
        ")\n",
        "load_job.result()\n",
        "print(f\"‚úì Table restored from backup\")\n",
        "```\n",
        "\n",
        "### Option 2: Query to remove appended rows\n",
        "\n",
        "If you know a way to identify the appended rows (e.g., by timestamp), you can use SQL to delete them:\n",
        "\n",
        "```sql\n",
        "DELETE FROM `project.dataset.table`\n",
        "WHERE condition_to_identify_new_rows;\n",
        "```\n",
        "\n",
        "The backup location was printed in the backup cell above.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mpg-data-warehouse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
