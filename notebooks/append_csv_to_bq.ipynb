{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Append CSV to BigQuery Table\n",
        "\n",
        "This notebook appends data from a CSV file in GCS to an existing BigQuery table.\n",
        "\n",
        "## Features\n",
        "- ‚úÖ Reads CSV from Google Cloud Storage\n",
        "- ‚úÖ Creates backup of existing BigQuery table before appending\n",
        "- ‚úÖ Appends new data to existing table (WRITE_APPEND mode)\n",
        "- ‚úÖ Validates data integrity after append\n",
        "- ‚úÖ Provides detailed summary report\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: `config.yml` with `csv_append` section configured\n",
        "- Required packages: google-cloud-bigquery, google-cloud-storage, pandas, pyyaml\n",
        "\n",
        "## Configuration\n",
        "Update `config.yml` with your specific values in the `csv_append` section:\n",
        "```yaml\n",
        "csv_append:\n",
        "  gcs:\n",
        "    csv_url: \"gs://your-bucket/path/to/data.csv\"\n",
        "    backup_bucket: \"your-bucket\"\n",
        "    backup_prefix: \"backups/csv_append\"\n",
        "  bigquery:\n",
        "    table_id: \"your-project.your_dataset.your_table\"\n",
        "    project: \"your-project-id\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n",
        "\n",
        "Load settings from `config.yml` including:\n",
        "- CSV source URL in GCS\n",
        "- BigQuery table information\n",
        "- Backup location settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values\n",
        "GCS_CSV_URL = config['csv_append']['gcs']['csv_url']\n",
        "BACKUP_BUCKET = config['csv_append']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['csv_append']['gcs'].get('backup_prefix', 'backups')\n",
        "BQ_TABLE_ID = config['csv_append']['bigquery']['table_id']\n",
        "BQ_PROJECT = config['csv_append']['bigquery'].get('project')\n",
        "\n",
        "# Verify required config values\n",
        "if not GCS_CSV_URL or GCS_CSV_URL.startswith('gs://your-'):\n",
        "    raise ValueError(\"Please configure csv_append.gcs.csv_url in config.yml\")\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure csv_append.bigquery.table_id in config.yml\")\n",
        "\n",
        "print(\"‚úì Configuration loaded successfully\")\n",
        "print(f\"  CSV URL: {GCS_CSV_URL[:50]}...\" if len(GCS_CSV_URL) > 50 else f\"  CSV URL: {GCS_CSV_URL}\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Google Cloud clients\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "storage_client = storage.Client(project=BQ_PROJECT) if BQ_PROJECT else storage.Client()\n",
        "\n",
        "print(f\"‚úì Clients initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Existing BigQuery Table\n",
        "\n",
        "Load the current table to:\n",
        "- Verify it exists\n",
        "- Get row count before append\n",
        "- Prepare for backup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read existing data from BigQuery\n",
        "print(f\"Reading existing data from {BQ_TABLE_ID}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "\n",
        "try:\n",
        "    df_existing = bq_client.query(query).to_dataframe()\n",
        "    print(f\"‚úì Existing table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing)}\")\n",
        "    print(f\"  Columns: {list(df_existing.columns)}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(df_existing.head())\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Error reading table: {e}\")\n",
        "    print(\"  The table must exist before appending data.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backup Existing Table to GCS\n",
        "\n",
        "‚ö†Ô∏è **CRITICAL STEP**: Create a backup of the existing table before appending new data.\n",
        "\n",
        "This backup can be used to restore the table if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backup existing table to GCS\n",
        "if BACKUP_BUCKET:\n",
        "    # Generate backup path with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = f\"gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/{timestamp}/*.csv\"\n",
        "    \n",
        "    print(f\"Creating backup of existing table...\")\n",
        "    print(f\"  Source table: {BQ_TABLE_ID}\")\n",
        "    print(f\"  Destination: {backup_path}\")\n",
        "    print(f\"  Rows to backup: {len(df_existing)}\")\n",
        "    \n",
        "    # Export table to GCS\n",
        "    extract_job = bq_client.extract_table(\n",
        "        BQ_TABLE_ID,\n",
        "        backup_path,\n",
        "        location=\"US\"\n",
        "    )\n",
        "    \n",
        "    # Wait for job to complete\n",
        "    extract_job.result()\n",
        "    \n",
        "    print(f\"\\n‚úì Backup completed successfully\")\n",
        "    print(f\"  Job ID: {extract_job.job_id}\")\n",
        "    print(f\"  Backup location: {backup_path}\")\n",
        "    \n",
        "    # Store backup info for later reference\n",
        "    BACKUP_LOCATION = backup_path\n",
        "    BACKUP_TIMESTAMP = timestamp\n",
        "else:\n",
        "    print(\"‚ö† Backup bucket not configured in config.yml\")\n",
        "    print(\"  Set 'csv_append.gcs.backup_bucket' to enable automatic backups\")\n",
        "    print(\"  Proceeding without backup...\")\n",
        "    BACKUP_LOCATION = None\n",
        "    BACKUP_TIMESTAMP = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read CSV from GCS\n",
        "\n",
        "Load the new data to be appended to the BigQuery table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV from GCS\n",
        "print(f\"Reading CSV from GCS...\")\n",
        "print(f\"  Source: {GCS_CSV_URL}\")\n",
        "\n",
        "try:\n",
        "    # Try UTF-8 first, fallback to latin-1 if needed\n",
        "    df_new = pd.read_csv(GCS_CSV_URL, encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"  Note: Using latin-1 encoding to handle special characters\")\n",
        "    df_new = pd.read_csv(GCS_CSV_URL, encoding='latin-1')\n",
        "\n",
        "print(f\"\\n‚úì CSV loaded successfully:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {list(df_new.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_new.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Schema Compatibility\n",
        "\n",
        "Verify that the CSV columns match the existing BigQuery table schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate schema compatibility\n",
        "print(\"=== Schema Validation ===\\n\")\n",
        "\n",
        "# Check column names\n",
        "existing_cols = set(df_existing.columns)\n",
        "new_cols = set(df_new.columns)\n",
        "\n",
        "if existing_cols == new_cols:\n",
        "    print(f\"‚úì Column names match ({len(new_cols)} columns)\")\n",
        "else:\n",
        "    print(\"‚ö† Column differences detected:\")\n",
        "    if new_cols - existing_cols:\n",
        "        print(f\"  Extra columns in CSV: {new_cols - existing_cols}\")\n",
        "    if existing_cols - new_cols:\n",
        "        print(f\"  Missing columns in CSV: {existing_cols - new_cols}\")\n",
        "    \n",
        "    user_input = input(\"\\nContinue anyway? (yes/no): \")\n",
        "    if user_input.lower() != 'yes':\n",
        "        raise ValueError(\"Schema mismatch - aborting append operation\")\n",
        "\n",
        "print(f\"\\nColumns: {list(df_new.columns)}\")\n",
        "\n",
        "# Check data types\n",
        "print(f\"\\nData type comparison:\")\n",
        "for col in df_new.columns:\n",
        "    if col in df_existing.columns:\n",
        "        existing_type = str(df_existing[col].dtype)\n",
        "        new_type = str(df_new[col].dtype)\n",
        "        match_symbol = \"‚úì\" if existing_type == new_type else \"‚ö†\"\n",
        "        print(f\"  {match_symbol} {col:30s} existing: {existing_type:10s} ‚Üí new: {new_type:10s}\")\n",
        "\n",
        "print(f\"\\n‚úì Schema validation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Check for Duplicates\n",
        "\n",
        "Check if any records in the new CSV already exist in the table.\n",
        "\n",
        "**Note**: To use this check, specify the key column(s) that uniquely identify records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Check for duplicate records\n",
        "# Uncomment and specify key columns to check for duplicates\n",
        "\n",
        "# KEY_COLUMNS = ['column1', 'column2']  # Specify columns that uniquely identify records\n",
        "\n",
        "# if 'KEY_COLUMNS' in locals() and KEY_COLUMNS:\n",
        "#     print(\"Checking for duplicates based on:\", KEY_COLUMNS)\n",
        "#     \n",
        "#     # Check if key columns exist\n",
        "#     missing_cols = set(KEY_COLUMNS) - set(df_new.columns)\n",
        "#     if missing_cols:\n",
        "#         print(f\"‚ö† Key columns not found in CSV: {missing_cols}\")\n",
        "#     else:\n",
        "#         # Find duplicates between existing and new data\n",
        "#         existing_keys = df_existing[KEY_COLUMNS].apply(tuple, axis=1).tolist()\n",
        "#         new_keys = df_new[KEY_COLUMNS].apply(tuple, axis=1).tolist()\n",
        "#         \n",
        "#         duplicates = set(new_keys) & set(existing_keys)\n",
        "#         \n",
        "#         if duplicates:\n",
        "#             print(f\"‚ö† Found {len(duplicates)} duplicate records in new data\")\n",
        "#             print(f\"  These records already exist in the table\")\n",
        "#             \n",
        "#             # Show duplicate records\n",
        "#             dup_mask = df_new[KEY_COLUMNS].apply(tuple, axis=1).isin(duplicates)\n",
        "#             print(f\"\\nDuplicate records:\")\n",
        "#             display(df_new[dup_mask])\n",
        "#             \n",
        "#             user_input = input(\"\\nContinue with append (duplicates will be added)? (yes/no): \")\n",
        "#             if user_input.lower() != 'yes':\n",
        "#                 raise ValueError(\"Duplicate check failed - aborting append operation\")\n",
        "#         else:\n",
        "#             print(\"‚úì No duplicates found - all records are new\")\n",
        "\n",
        "print(\"‚úì Duplicate check skipped (configure KEY_COLUMNS to enable)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preview Data Comparison\n",
        "\n",
        "Compare existing and new data before appending.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display data summary\n",
        "print(\"=== Data Summary ===\\n\")\n",
        "print(f\"Existing table:\")\n",
        "print(f\"  Rows: {len(df_existing)}\")\n",
        "print(f\"  Columns: {len(df_existing.columns)}\")\n",
        "\n",
        "print(f\"\\nNew data to append:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {len(df_new.columns)}\")\n",
        "\n",
        "print(f\"\\nAfter append:\")\n",
        "print(f\"  Expected total rows: {len(df_existing) + len(df_new)}\")\n",
        "\n",
        "print(\"\\n--- Existing Data Sample ---\")\n",
        "display(df_existing.head(3))\n",
        "\n",
        "print(\"\\n--- New Data Sample ---\")\n",
        "display(df_new.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Append Data to BigQuery Table\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT**: This will append new rows to the existing table.\n",
        "\n",
        "The backup has been created. Review the data above before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Append data to BigQuery table\n",
        "print(\"=\" * 60)\n",
        "print(\"APPENDING DATA TO BIGQUERY TABLE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTable: {BQ_TABLE_ID}\")\n",
        "print(f\"Rows to append: {len(df_new)}\")\n",
        "print(f\"Current rows: {len(df_existing)}\")\n",
        "print(f\"Mode: WRITE_APPEND (add to existing table)\")\n",
        "print(f\"\\nStarting append at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "\n",
        "# Configure job to append to existing table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_APPEND\"  # Append to existing table\n",
        ")\n",
        "\n",
        "# Load dataframe to BigQuery\n",
        "load_job = bq_client.load_table_from_dataframe(\n",
        "    df_new,\n",
        "    BQ_TABLE_ID,\n",
        "    job_config=job_config\n",
        ")\n",
        "\n",
        "# Wait for job to complete\n",
        "load_job.result()\n",
        "\n",
        "print(f\"\\n‚úì Append completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"  Rows appended: {load_job.output_rows}\")\n",
        "print(f\"  Job ID: {load_job.job_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Append Operation\n",
        "\n",
        "Read back the table to verify the append was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read updated table\n",
        "print(\"Verifying append operation...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "df_updated = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"\\n‚úì Verification complete\")\n",
        "print(f\"  Rows in table: {len(df_updated)}\")\n",
        "print(f\"  Columns: {list(df_updated.columns)}\")\n",
        "print(f\"\\nLast few rows of updated table (should include new data):\")\n",
        "df_updated.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify row counts\n",
        "print(\"Data integrity check:\")\n",
        "print(f\"  Rows before append:  {len(df_existing)}\")\n",
        "print(f\"  Rows appended:       {len(df_new)}\")\n",
        "print(f\"  Expected total:      {len(df_existing) + len(df_new)}\")\n",
        "print(f\"  Actual rows in table: {len(df_updated)}\")\n",
        "\n",
        "if len(df_updated) == len(df_existing) + len(df_new):\n",
        "    print(f\"\\n‚úì Row count verified - all {len(df_new)} rows successfully appended\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† Row count mismatch!\")\n",
        "    print(f\"  Expected: {len(df_existing) + len(df_new)}\")\n",
        "    print(f\"  Actual:   {len(df_updated)}\")\n",
        "    print(f\"  Difference: {len(df_updated) - (len(df_existing) + len(df_new))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report\n",
        "\n",
        "Complete summary of the append operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"CSV APPEND TO BIGQUERY - SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\nüìÇ Source:\")\n",
        "print(f\"  CSV: {GCS_CSV_URL.split('/')[-1]}\")\n",
        "print(f\"  Location: {'/'.join(GCS_CSV_URL.split('/')[:-1])}\")\n",
        "\n",
        "print(f\"\\nüéØ Target:\")\n",
        "print(f\"  Table: {BQ_TABLE_ID}\")\n",
        "print(f\"  Project: {bq_client.project}\")\n",
        "\n",
        "print(f\"\\nüìä Data Changes:\")\n",
        "print(f\"  Rows before:  {len(df_existing)}\")\n",
        "print(f\"  Rows added:   {len(df_new)}\")\n",
        "print(f\"  Rows after:   {len(df_updated)}\")\n",
        "print(f\"  Net change:   +{len(df_updated) - len(df_existing)}\")\n",
        "\n",
        "if BACKUP_LOCATION:\n",
        "    print(f\"\\nüíæ Backup:\")\n",
        "    print(f\"  Location: {BACKUP_LOCATION}\")\n",
        "    print(f\"  Timestamp: {BACKUP_TIMESTAMP}\")\n",
        "    print(f\"  Status: ‚úì Created before append\")\n",
        "else:\n",
        "    print(f\"\\nüíæ Backup:\")\n",
        "    print(f\"  Status: ‚ö† No backup created\")\n",
        "\n",
        "print(f\"\\n‚úÖ Append completed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollback Instructions (If Needed)\n",
        "\n",
        "If you need to rollback to the previous version, restore from the backup created at the beginning of this notebook.\n",
        "\n",
        "### Option 1: Restore from BigQuery backup\n",
        "\n",
        "```python\n",
        "# Replace table with backup data\n",
        "backup_path = \"gs://BUCKET/PREFIX/TIMESTAMP/*.csv\"\n",
        "df_backup = pd.read_csv(backup_path)\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  # Replace entire table\n",
        ")\n",
        "\n",
        "load_job = bq_client.load_table_from_dataframe(\n",
        "    df_backup,\n",
        "    BQ_TABLE_ID,\n",
        "    job_config=job_config\n",
        ")\n",
        "load_job.result()\n",
        "print(f\"‚úì Table restored from backup\")\n",
        "```\n",
        "\n",
        "### Option 2: Query to remove appended rows\n",
        "\n",
        "If you know a way to identify the appended rows (e.g., by timestamp), you can use SQL to delete them:\n",
        "\n",
        "```sql\n",
        "DELETE FROM `project.dataset.table`\n",
        "WHERE condition_to_identify_new_rows;\n",
        "```\n",
        "\n",
        "The backup location was printed in the backup cell above.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mpg-data-warehouse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
