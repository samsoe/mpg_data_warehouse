{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Update gridVeg Point Intercepts in BigQuery\n",
        "\n",
        "This notebook appends new point intercept data to BigQuery tables from a CSV file stored in GCS.\n",
        "\n",
        "**Operation**: APPEND new rows (not replace entire table)\n",
        "\n",
        "**Target Tables**:\n",
        "- `gridVeg_point_intercept_vegetation` - vegetation intercept data (4 height layers)\n",
        "- `gridVeg_point_intercept_ground` - ground cover intercept data\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, google-cloud-storage, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration loaded successfully\n",
            "  CSV URL: gs://mpg-data-warehouse/gridVeg/src/2025/2025-09-18_gridVeg_...\n",
            "  Vegetation Table: mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_point_intercept_vegetation\n",
            "  Ground Table: mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_point_intercept_ground\n",
            "  Backup: gs://mpg-data-warehouse/gridVeg/bak\n"
          ]
        }
      ],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values for gridVeg point intercepts\n",
        "GCS_CSV_URL = config['gridveg_point_intercepts']['gcs']['csv_url']\n",
        "BACKUP_BUCKET = config['gridveg_point_intercepts']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gridveg_point_intercepts']['gcs'].get('backup_prefix', 'backups/gridveg_point_intercepts')\n",
        "\n",
        "BQ_TABLE_VEGETATION = config['gridveg_point_intercepts']['bigquery']['table_vegetation']\n",
        "BQ_TABLE_GROUND = config['gridveg_point_intercepts']['bigquery']['table_ground']\n",
        "BQ_PROJECT = config['gridveg_point_intercepts']['bigquery'].get('project')\n",
        "\n",
        "# Verify required config values\n",
        "if not GCS_CSV_URL or GCS_CSV_URL.startswith('gs://your-'):\n",
        "    raise ValueError(\"Please configure gridveg_point_intercepts.gcs.csv_url in config.yml\")\n",
        "if not BQ_TABLE_VEGETATION or 'your-project' in BQ_TABLE_VEGETATION:\n",
        "    raise ValueError(\"Please configure gridveg_point_intercepts.bigquery.table_vegetation in config.yml\")\n",
        "if not BQ_TABLE_GROUND or 'your-project' in BQ_TABLE_GROUND:\n",
        "    raise ValueError(\"Please configure gridveg_point_intercepts.bigquery.table_ground in config.yml\")\n",
        "\n",
        "print(\"✓ Configuration loaded successfully\")\n",
        "print(f\"  CSV URL: {GCS_CSV_URL[:60]}...\" if len(GCS_CSV_URL) > 60 else f\"  CSV URL: {GCS_CSV_URL}\")\n",
        "print(f\"  Vegetation Table: {BQ_TABLE_VEGETATION}\")\n",
        "print(f\"  Ground Table: {BQ_TABLE_GROUND}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Clients initialized\n",
            "  Project: mpg-data-warehouse\n"
          ]
        }
      ],
      "source": [
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "storage_client = storage.Client(project=BQ_PROJECT) if BQ_PROJECT else storage.Client()\n",
        "\n",
        "print(f\"✓ Clients initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading CSV from GCS...\n",
            "✓ CSV loaded successfully:\n",
            "  Rows: 7799\n",
            "  Columns: ['Survey Data::__kp_Survey', 'Survey Data::_kf_Site', 'Survey Data::SurveyDate', 'Survey Data::SurveyYear', 'PointTrans', '_kf_Hit1_serial', 'Height', '_kf_Hit2_serial', '_kf_Hit3_serial', '_kf_Hit4_serial', 'GroundCover']\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survey Data::__kp_Survey</th>\n",
              "      <th>Survey Data::_kf_Site</th>\n",
              "      <th>Survey Data::SurveyDate</th>\n",
              "      <th>Survey Data::SurveyYear</th>\n",
              "      <th>PointTrans</th>\n",
              "      <th>_kf_Hit1_serial</th>\n",
              "      <th>Height</th>\n",
              "      <th>_kf_Hit2_serial</th>\n",
              "      <th>_kf_Hit3_serial</th>\n",
              "      <th>_kf_Hit4_serial</th>\n",
              "      <th>GroundCover</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>5/21/25</td>\n",
              "      <td>2025</td>\n",
              "      <td>N1</td>\n",
              "      <td>529</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>L</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>5/21/25</td>\n",
              "      <td>2025</td>\n",
              "      <td>N2</td>\n",
              "      <td>360</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>L</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>5/21/25</td>\n",
              "      <td>2025</td>\n",
              "      <td>N3</td>\n",
              "      <td>529</td>\n",
              "      <td>NaN</td>\n",
              "      <td>405.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>L</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>5/21/25</td>\n",
              "      <td>2025</td>\n",
              "      <td>N4</td>\n",
              "      <td>529</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WDL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>5/21/25</td>\n",
              "      <td>2025</td>\n",
              "      <td>N5</td>\n",
              "      <td>334</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WDL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Survey Data::__kp_Survey  Survey Data::_kf_Site  \\\n",
              "0  B45700C5-D391-4679-8579-217DCB1385A2                    227   \n",
              "1  B45700C5-D391-4679-8579-217DCB1385A2                    227   \n",
              "2  B45700C5-D391-4679-8579-217DCB1385A2                    227   \n",
              "3  B45700C5-D391-4679-8579-217DCB1385A2                    227   \n",
              "4  B45700C5-D391-4679-8579-217DCB1385A2                    227   \n",
              "\n",
              "  Survey Data::SurveyDate  Survey Data::SurveyYear PointTrans  \\\n",
              "0                 5/21/25                     2025         N1   \n",
              "1                 5/21/25                     2025         N2   \n",
              "2                 5/21/25                     2025         N3   \n",
              "3                 5/21/25                     2025         N4   \n",
              "4                 5/21/25                     2025         N5   \n",
              "\n",
              "   _kf_Hit1_serial  Height  _kf_Hit2_serial  _kf_Hit3_serial  _kf_Hit4_serial  \\\n",
              "0              529     NaN              NaN              NaN              NaN   \n",
              "1              360     NaN              NaN              NaN              NaN   \n",
              "2              529     NaN            405.0              NaN              NaN   \n",
              "3              529     NaN              NaN              NaN              NaN   \n",
              "4              334     NaN              NaN              NaN              NaN   \n",
              "\n",
              "  GroundCover  \n",
              "0           L  \n",
              "1           L  \n",
              "2           L  \n",
              "3         WDL  \n",
              "4         WDL  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read CSV from GCS (new data)\n",
        "print(\"Reading CSV from GCS...\")\n",
        "df_new = pd.read_csv(GCS_CSV_URL)\n",
        "\n",
        "print(f\"✓ CSV loaded successfully:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {list(df_new.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_new.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform CSV Data\n",
        "\n",
        "The source CSV will be transformed into two separate datasets:\n",
        "\n",
        "### Vegetation Table Transformations\n",
        "- Rename columns to match BigQuery schema\n",
        "- Convert date format from mm/dd/yy to ISO format (YYYY-MM-DD)\n",
        "- Filter out 2010 records (per requirements)\n",
        "- Select columns: survey_ID, grid_point, date, year, transect_point, height_intercept_1, intercept_1-4\n",
        "\n",
        "### Ground Table Transformations\n",
        "- Rename columns to match BigQuery schema  \n",
        "- Convert date format from mm/dd/yy to ISO format (YYYY-MM-DD)\n",
        "- Filter out 2010 records (per requirements)\n",
        "- Select columns: survey_ID, grid_point, date, year, transect_point, intercept_1, intercept_ground_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vegetation table column mapping:\n",
            "  Survey Data::__kp_Survey       → survey_ID\n",
            "  Survey Data::_kf_Site          → grid_point\n",
            "  Survey Data::SurveyDate        → date\n",
            "  Survey Data::SurveyYear        → year\n",
            "  PointTrans                     → transect_point\n",
            "  Height                         → height_intercept_1\n",
            "  _kf_Hit1_serial                → intercept_1\n",
            "  _kf_Hit2_serial                → intercept_2\n",
            "  _kf_Hit3_serial                → intercept_3\n",
            "  _kf_Hit4_serial                → intercept_4\n"
          ]
        }
      ],
      "source": [
        "# Define column mapping for VEGETATION table\n",
        "column_mapping_vegetation = {\n",
        "    'Survey Data::__kp_Survey': 'survey_ID',\n",
        "    'Survey Data::_kf_Site': 'grid_point',\n",
        "    'Survey Data::SurveyDate': 'date',\n",
        "    'Survey Data::SurveyYear': 'year',\n",
        "    'PointTrans': 'transect_point',\n",
        "    'Height': 'height_intercept_1',\n",
        "    '_kf_Hit1_serial': 'intercept_1',\n",
        "    '_kf_Hit2_serial': 'intercept_2',\n",
        "    '_kf_Hit3_serial': 'intercept_3',\n",
        "    '_kf_Hit4_serial': 'intercept_4'\n",
        "}\n",
        "\n",
        "print(\"Vegetation table column mapping:\")\n",
        "for csv_col, bq_col in column_mapping_vegetation.items():\n",
        "    print(f\"  {csv_col:30s} → {bq_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground table column mapping:\n",
            "  Survey Data::__kp_Survey       → survey_ID\n",
            "  Survey Data::_kf_Site          → grid_point\n",
            "  Survey Data::SurveyDate        → date\n",
            "  Survey Data::SurveyYear        → year\n",
            "  PointTrans                     → transect_point\n",
            "  _kf_Hit1_serial                → intercept_1\n",
            "  GroundCover                    → intercept_ground_code\n"
          ]
        }
      ],
      "source": [
        "# Define column mapping for GROUND table\n",
        "column_mapping_ground = {\n",
        "    'Survey Data::__kp_Survey': 'survey_ID',\n",
        "    'Survey Data::_kf_Site': 'grid_point',\n",
        "    'Survey Data::SurveyDate': 'date',\n",
        "    'Survey Data::SurveyYear': 'year',\n",
        "    'PointTrans': 'transect_point',\n",
        "    '_kf_Hit1_serial': 'intercept_1',\n",
        "    'GroundCover': 'intercept_ground_code'\n",
        "}\n",
        "\n",
        "print(\"Ground table column mapping:\")\n",
        "for csv_col, bq_col in column_mapping_ground.items():\n",
        "    print(f\"  {csv_col:30s} → {bq_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRANSFORMING DATA FOR VEGETATION TABLE\n",
            "============================================================\n",
            "\n",
            "✓ Columns renamed\n",
            "  Transformed columns: ['survey_ID', 'grid_point', 'date', 'year', 'transect_point', 'height_intercept_1', 'intercept_1', 'intercept_2', 'intercept_3', 'intercept_4']\n",
            "✓ Date format converted to YYYY-MM-DD string\n",
            "  Sample dates: ['2025-05-21', '2025-05-21', '2025-05-21']\n",
            "\n",
            "✓ Filtered out 2010 records\n",
            "  Rows before: 7799\n",
            "  Rows after:  7799\n",
            "  Removed:     0\n",
            "\n",
            "Vegetation data preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>transect_point</th>\n",
              "      <th>height_intercept_1</th>\n",
              "      <th>intercept_1</th>\n",
              "      <th>intercept_2</th>\n",
              "      <th>intercept_3</th>\n",
              "      <th>intercept_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>529</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>360</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>529</td>\n",
              "      <td>405.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>529</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>334</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              survey_ID  grid_point        date  year  \\\n",
              "0  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "1  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "2  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "3  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "4  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "\n",
              "  transect_point  height_intercept_1  intercept_1  intercept_2  intercept_3  \\\n",
              "0             N1                 NaN          529          NaN          NaN   \n",
              "1             N2                 NaN          360          NaN          NaN   \n",
              "2             N3                 NaN          529        405.0          NaN   \n",
              "3             N4                 NaN          529          NaN          NaN   \n",
              "4             N5                 NaN          334          NaN          NaN   \n",
              "\n",
              "   intercept_4  \n",
              "0          NaN  \n",
              "1          NaN  \n",
              "2          NaN  \n",
              "3          NaN  \n",
              "4          NaN  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform data for VEGETATION table\n",
        "print(\"=\" * 60)\n",
        "print(\"TRANSFORMING DATA FOR VEGETATION TABLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Select and rename columns\n",
        "df_vegetation = df_new[list(column_mapping_vegetation.keys())].copy()\n",
        "df_vegetation = df_vegetation.rename(columns=column_mapping_vegetation)\n",
        "\n",
        "print(f\"\\n✓ Columns renamed\")\n",
        "print(f\"  Transformed columns: {list(df_vegetation.columns)}\")\n",
        "\n",
        "# Convert date from m/d/yy to YYYY-MM-DD string format for CSV upload\n",
        "df_vegetation['date'] = pd.to_datetime(df_vegetation['date'], format='%m/%d/%y').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "print(f\"✓ Date format converted to YYYY-MM-DD string\")\n",
        "print(f\"  Sample dates: {df_vegetation['date'].head(3).tolist()}\")\n",
        "\n",
        "# Filter out 2010 records\n",
        "rows_before = len(df_vegetation)\n",
        "df_vegetation = df_vegetation[df_vegetation['year'] != 2010].copy()\n",
        "rows_after = len(df_vegetation)\n",
        "\n",
        "print(f\"\\n✓ Filtered out 2010 records\")\n",
        "print(f\"  Rows before: {rows_before}\")\n",
        "print(f\"  Rows after:  {rows_after}\")\n",
        "print(f\"  Removed:     {rows_before - rows_after}\")\n",
        "\n",
        "print(f\"\\nVegetation data preview:\")\n",
        "df_vegetation.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRANSFORMING DATA FOR GROUND TABLE\n",
            "============================================================\n",
            "\n",
            "✓ Columns renamed\n",
            "  Transformed columns: ['survey_ID', 'grid_point', 'date', 'year', 'transect_point', 'intercept_1', 'intercept_ground_code']\n",
            "✓ Date format converted to YYYY-MM-DD string\n",
            "  Sample dates: ['2025-05-21', '2025-05-21', '2025-05-21']\n",
            "\n",
            "✓ Filtered out 2010 records\n",
            "  Rows before: 7799\n",
            "  Rows after:  7799\n",
            "  Removed:     0\n",
            "\n",
            "Ground data preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>transect_point</th>\n",
              "      <th>intercept_1</th>\n",
              "      <th>intercept_ground_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N1</td>\n",
              "      <td>529</td>\n",
              "      <td>L</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N2</td>\n",
              "      <td>360</td>\n",
              "      <td>L</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N3</td>\n",
              "      <td>529</td>\n",
              "      <td>L</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N4</td>\n",
              "      <td>529</td>\n",
              "      <td>WDL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025-05-21</td>\n",
              "      <td>2025</td>\n",
              "      <td>N5</td>\n",
              "      <td>334</td>\n",
              "      <td>WDL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              survey_ID  grid_point        date  year  \\\n",
              "0  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "1  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "2  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "3  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "4  B45700C5-D391-4679-8579-217DCB1385A2         227  2025-05-21  2025   \n",
              "\n",
              "  transect_point  intercept_1 intercept_ground_code  \n",
              "0             N1          529                     L  \n",
              "1             N2          360                     L  \n",
              "2             N3          529                     L  \n",
              "3             N4          529                   WDL  \n",
              "4             N5          334                   WDL  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform data for GROUND table\n",
        "print(\"=\" * 60)\n",
        "print(\"TRANSFORMING DATA FOR GROUND TABLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Select and rename columns\n",
        "df_ground = df_new[list(column_mapping_ground.keys())].copy()\n",
        "df_ground = df_ground.rename(columns=column_mapping_ground)\n",
        "\n",
        "print(f\"\\n✓ Columns renamed\")\n",
        "print(f\"  Transformed columns: {list(df_ground.columns)}\")\n",
        "\n",
        "# Convert date from m/d/yy to YYYY-MM-DD string format for CSV upload\n",
        "df_ground['date'] = pd.to_datetime(df_ground['date'], format='%m/%d/%y').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "print(f\"✓ Date format converted to YYYY-MM-DD string\")\n",
        "print(f\"  Sample dates: {df_ground['date'].head(3).tolist()}\")\n",
        "\n",
        "# Filter out 2010 records\n",
        "rows_before = len(df_ground)\n",
        "df_ground = df_ground[df_ground['year'] != 2010].copy()\n",
        "rows_after = len(df_ground)\n",
        "\n",
        "print(f\"\\n✓ Filtered out 2010 records\")\n",
        "print(f\"  Rows before: {rows_before}\")\n",
        "print(f\"  Rows after:  {rows_after}\")\n",
        "print(f\"  Removed:     {rows_before - rows_after}\")\n",
        "\n",
        "print(f\"\\nGround data preview:\")\n",
        "df_ground.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Existing BigQuery Tables\n",
        "\n",
        "Load the current data from both BigQuery tables to compare with the new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading existing VEGETATION data from mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_point_intercept_vegetation...\n",
            "✓ Existing vegetation table loaded:\n",
            "  Rows: 291045\n",
            "  Columns: ['survey_ID', 'grid_point', 'date', 'year', 'transect_point', 'height_intercept_1', 'intercept_1', 'intercept_2', 'intercept_3', 'intercept_4']\n",
            "\n",
            "Existing vegetation data preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>transect_point</th>\n",
              "      <th>height_intercept_1</th>\n",
              "      <th>intercept_1</th>\n",
              "      <th>intercept_2</th>\n",
              "      <th>intercept_3</th>\n",
              "      <th>intercept_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01E14610-A291-4929-98A9-687132272D70</td>\n",
              "      <td>306</td>\n",
              "      <td>2023-05-24</td>\n",
              "      <td>2023</td>\n",
              "      <td>N3</td>\n",
              "      <td>None</td>\n",
              "      <td>525</td>\n",
              "      <td>12</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01E14610-A291-4929-98A9-687132272D70</td>\n",
              "      <td>306</td>\n",
              "      <td>2023-05-24</td>\n",
              "      <td>2023</td>\n",
              "      <td>N23</td>\n",
              "      <td>None</td>\n",
              "      <td>525</td>\n",
              "      <td>5</td>\n",
              "      <td>529</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01E14610-A291-4929-98A9-687132272D70</td>\n",
              "      <td>306</td>\n",
              "      <td>2023-05-24</td>\n",
              "      <td>2023</td>\n",
              "      <td>N30</td>\n",
              "      <td>None</td>\n",
              "      <td>525</td>\n",
              "      <td>5</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01E14610-A291-4929-98A9-687132272D70</td>\n",
              "      <td>306</td>\n",
              "      <td>2023-05-24</td>\n",
              "      <td>2023</td>\n",
              "      <td>E5</td>\n",
              "      <td>None</td>\n",
              "      <td>525</td>\n",
              "      <td>67</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01E14610-A291-4929-98A9-687132272D70</td>\n",
              "      <td>306</td>\n",
              "      <td>2023-05-24</td>\n",
              "      <td>2023</td>\n",
              "      <td>E17</td>\n",
              "      <td>None</td>\n",
              "      <td>525</td>\n",
              "      <td>12</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              survey_ID  grid_point        date  year  \\\n",
              "0  01E14610-A291-4929-98A9-687132272D70         306  2023-05-24  2023   \n",
              "1  01E14610-A291-4929-98A9-687132272D70         306  2023-05-24  2023   \n",
              "2  01E14610-A291-4929-98A9-687132272D70         306  2023-05-24  2023   \n",
              "3  01E14610-A291-4929-98A9-687132272D70         306  2023-05-24  2023   \n",
              "4  01E14610-A291-4929-98A9-687132272D70         306  2023-05-24  2023   \n",
              "\n",
              "  transect_point height_intercept_1  intercept_1  intercept_2  intercept_3  \\\n",
              "0             N3               None          525           12         <NA>   \n",
              "1            N23               None          525            5          529   \n",
              "2            N30               None          525            5         <NA>   \n",
              "3             E5               None          525           67         <NA>   \n",
              "4            E17               None          525           12         <NA>   \n",
              "\n",
              "   intercept_4  \n",
              "0         <NA>  \n",
              "1         <NA>  \n",
              "2         <NA>  \n",
              "3         <NA>  \n",
              "4         <NA>  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Read existing VEGETATION table from BigQuery\n",
        "print(f\"Reading existing VEGETATION data from {BQ_TABLE_VEGETATION}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_VEGETATION}`\"\n",
        "\n",
        "try:\n",
        "    df_existing_vegetation = bq_client.query(query).to_dataframe()\n",
        "    print(f\"✓ Existing vegetation table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing_vegetation)}\")\n",
        "    print(f\"  Columns: {list(df_existing_vegetation.columns)}\")\n",
        "    print(f\"\\nExisting vegetation data preview:\")\n",
        "    display(df_existing_vegetation.head())\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error reading table: {e}\")\n",
        "    print(\"  This may be expected if the table doesn't exist yet.\")\n",
        "    df_existing_vegetation = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading existing GROUND data from mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_point_intercept_ground...\n",
            "✓ Existing ground table loaded:\n",
            "  Rows: 291045\n",
            "  Columns: ['survey_ID', 'grid_point', 'date', 'year', 'transect_point', 'intercept_1', 'intercept_ground_code']\n",
            "\n",
            "Existing ground data preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survey_ID</th>\n",
              "      <th>grid_point</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>transect_point</th>\n",
              "      <th>intercept_1</th>\n",
              "      <th>intercept_ground_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0B3C78C3-5612-4BC3-A14F-B4D7A7041C35</td>\n",
              "      <td>586</td>\n",
              "      <td>2022-05-16</td>\n",
              "      <td>2022</td>\n",
              "      <td>E29</td>\n",
              "      <td>360</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0B3C78C3-5612-4BC3-A14F-B4D7A7041C35</td>\n",
              "      <td>586</td>\n",
              "      <td>2022-05-16</td>\n",
              "      <td>2022</td>\n",
              "      <td>E41</td>\n",
              "      <td>360</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0B3C78C3-5612-4BC3-A14F-B4D7A7041C35</td>\n",
              "      <td>586</td>\n",
              "      <td>2022-05-16</td>\n",
              "      <td>2022</td>\n",
              "      <td>E43</td>\n",
              "      <td>360</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0B3C78C3-5612-4BC3-A14F-B4D7A7041C35</td>\n",
              "      <td>586</td>\n",
              "      <td>2022-05-16</td>\n",
              "      <td>2022</td>\n",
              "      <td>E46</td>\n",
              "      <td>360</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0B3C78C3-5612-4BC3-A14F-B4D7A7041C35</td>\n",
              "      <td>586</td>\n",
              "      <td>2022-05-16</td>\n",
              "      <td>2022</td>\n",
              "      <td>E44</td>\n",
              "      <td>360</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              survey_ID  grid_point        date  year  \\\n",
              "0  0B3C78C3-5612-4BC3-A14F-B4D7A7041C35         586  2022-05-16  2022   \n",
              "1  0B3C78C3-5612-4BC3-A14F-B4D7A7041C35         586  2022-05-16  2022   \n",
              "2  0B3C78C3-5612-4BC3-A14F-B4D7A7041C35         586  2022-05-16  2022   \n",
              "3  0B3C78C3-5612-4BC3-A14F-B4D7A7041C35         586  2022-05-16  2022   \n",
              "4  0B3C78C3-5612-4BC3-A14F-B4D7A7041C35         586  2022-05-16  2022   \n",
              "\n",
              "  transect_point  intercept_1 intercept_ground_code  \n",
              "0            E29          360                  None  \n",
              "1            E41          360                  None  \n",
              "2            E43          360                  None  \n",
              "3            E46          360                  None  \n",
              "4            E44          360                  None  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Read existing GROUND table from BigQuery\n",
        "print(f\"Reading existing GROUND data from {BQ_TABLE_GROUND}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_GROUND}`\"\n",
        "\n",
        "try:\n",
        "    df_existing_ground = bq_client.query(query).to_dataframe()\n",
        "    print(f\"✓ Existing ground table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing_ground)}\")\n",
        "    print(f\"  Columns: {list(df_existing_ground.columns)}\")\n",
        "    print(f\"\\nExisting ground data preview:\")\n",
        "    display(df_existing_ground.head())\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error reading table: {e}\")\n",
        "    print(\"  This may be expected if the table doesn't exist yet.\")\n",
        "    df_existing_ground = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare New vs Existing Data\n",
        "\n",
        "Identify which rows in the new data are not already in the existing tables.\n",
        "\n",
        "**Unique identifier**: `survey_ID + transect_point` (each survey has multiple transect points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "COMPARING VEGETATION DATA\n",
            "============================================================\n",
            "\n",
            "Row count:\n",
            "  Existing: 291045\n",
            "  New CSV:  7799\n",
            "\n",
            "✓ Columns match (10 columns)\n",
            "\n",
            "✓ Found 7799 new vegetation records to append\n",
            "\n",
            "New records by year:\n",
            "  2025: 7799 records\n"
          ]
        }
      ],
      "source": [
        "# Compare VEGETATION datasets\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARING VEGETATION DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if df_existing_vegetation is not None:\n",
        "    print(f\"\\nRow count:\")\n",
        "    print(f\"  Existing: {len(df_existing_vegetation)}\")\n",
        "    print(f\"  New CSV:  {len(df_vegetation)}\")\n",
        "    \n",
        "    # Column comparison\n",
        "    existing_cols = set(df_existing_vegetation.columns)\n",
        "    new_cols = set(df_vegetation.columns)\n",
        "    \n",
        "    if existing_cols == new_cols:\n",
        "        print(f\"\\n✓ Columns match ({len(new_cols)} columns)\")\n",
        "    else:\n",
        "        print(\"\\n⚠ Column differences detected:\")\n",
        "        if new_cols - existing_cols:\n",
        "            print(f\"  New columns: {new_cols - existing_cols}\")\n",
        "        if existing_cols - new_cols:\n",
        "            print(f\"  Missing columns: {existing_cols - new_cols}\")\n",
        "    \n",
        "    # Create composite key for comparison\n",
        "    df_existing_vegetation['_composite_key'] = (\n",
        "        df_existing_vegetation['survey_ID'].astype(str) + '|' + \n",
        "        df_existing_vegetation['transect_point'].astype(str)\n",
        "    )\n",
        "    df_vegetation['_composite_key'] = (\n",
        "        df_vegetation['survey_ID'].astype(str) + '|' + \n",
        "        df_vegetation['transect_point'].astype(str)\n",
        "    )\n",
        "    \n",
        "    existing_keys = set(df_existing_vegetation['_composite_key'])\n",
        "    new_keys = set(df_vegetation['_composite_key'])\n",
        "    \n",
        "    # Find records to append\n",
        "    keys_to_append = new_keys - existing_keys\n",
        "    \n",
        "    if keys_to_append:\n",
        "        df_vegetation_to_append = df_vegetation[df_vegetation['_composite_key'].isin(keys_to_append)].copy()\n",
        "        # Drop the temporary composite key\n",
        "        df_vegetation_to_append = df_vegetation_to_append.drop(columns=['_composite_key'])\n",
        "        \n",
        "        print(f\"\\n✓ Found {len(df_vegetation_to_append)} new vegetation records to append\")\n",
        "        \n",
        "        # Show year breakdown\n",
        "        print(f\"\\nNew records by year:\")\n",
        "        year_counts = df_vegetation_to_append['year'].value_counts().sort_index()\n",
        "        for year, count in year_counts.items():\n",
        "            print(f\"  {year}: {count} records\")\n",
        "    else:\n",
        "        df_vegetation_to_append = None\n",
        "        print(\"\\n⚠ No new records found - all keys already exist in table\")\n",
        "    \n",
        "    # Check for duplicates\n",
        "    duplicate_keys = existing_keys & new_keys\n",
        "    if duplicate_keys:\n",
        "        print(f\"\\n⚠ Warning: {len(duplicate_keys)} records already exist in table\")\n",
        "        print(f\"  These will be skipped during append.\")\n",
        "else:\n",
        "    # No existing table, all records are new\n",
        "    df_vegetation_to_append = df_vegetation.copy()\n",
        "    print(f\"✓ No existing table - will create new table with {len(df_vegetation_to_append)} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "COMPARING GROUND DATA\n",
            "============================================================\n",
            "\n",
            "Row count:\n",
            "  Existing: 291045\n",
            "  New CSV:  7799\n",
            "\n",
            "✓ Columns match (7 columns)\n",
            "\n",
            "✓ Found 7799 new ground records to append\n",
            "\n",
            "New records by year:\n",
            "  2025: 7799 records\n"
          ]
        }
      ],
      "source": [
        "# Compare GROUND datasets\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARING GROUND DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if df_existing_ground is not None:\n",
        "    print(f\"\\nRow count:\")\n",
        "    print(f\"  Existing: {len(df_existing_ground)}\")\n",
        "    print(f\"  New CSV:  {len(df_ground)}\")\n",
        "    \n",
        "    # Column comparison\n",
        "    existing_cols = set(df_existing_ground.columns)\n",
        "    new_cols = set(df_ground.columns)\n",
        "    \n",
        "    if existing_cols == new_cols:\n",
        "        print(f\"\\n✓ Columns match ({len(new_cols)} columns)\")\n",
        "    else:\n",
        "        print(\"\\n⚠ Column differences detected:\")\n",
        "        if new_cols - existing_cols:\n",
        "            print(f\"  New columns: {new_cols - existing_cols}\")\n",
        "        if existing_cols - new_cols:\n",
        "            print(f\"  Missing columns: {existing_cols - new_cols}\")\n",
        "    \n",
        "    # Create composite key for comparison\n",
        "    df_existing_ground['_composite_key'] = (\n",
        "        df_existing_ground['survey_ID'].astype(str) + '|' + \n",
        "        df_existing_ground['transect_point'].astype(str)\n",
        "    )\n",
        "    df_ground['_composite_key'] = (\n",
        "        df_ground['survey_ID'].astype(str) + '|' + \n",
        "        df_ground['transect_point'].astype(str)\n",
        "    )\n",
        "    \n",
        "    existing_keys = set(df_existing_ground['_composite_key'])\n",
        "    new_keys = set(df_ground['_composite_key'])\n",
        "    \n",
        "    # Find records to append\n",
        "    keys_to_append = new_keys - existing_keys\n",
        "    \n",
        "    if keys_to_append:\n",
        "        df_ground_to_append = df_ground[df_ground['_composite_key'].isin(keys_to_append)].copy()\n",
        "        # Drop the temporary composite key\n",
        "        df_ground_to_append = df_ground_to_append.drop(columns=['_composite_key'])\n",
        "        \n",
        "        print(f\"\\n✓ Found {len(df_ground_to_append)} new ground records to append\")\n",
        "        \n",
        "        # Show year breakdown\n",
        "        print(f\"\\nNew records by year:\")\n",
        "        year_counts = df_ground_to_append['year'].value_counts().sort_index()\n",
        "        for year, count in year_counts.items():\n",
        "            print(f\"  {year}: {count} records\")\n",
        "    else:\n",
        "        df_ground_to_append = None\n",
        "        print(\"\\n⚠ No new records found - all keys already exist in table\")\n",
        "    \n",
        "    # Check for duplicates\n",
        "    duplicate_keys = existing_keys & new_keys\n",
        "    if duplicate_keys:\n",
        "        print(f\"\\n⚠ Warning: {len(duplicate_keys)} records already exist in table\")\n",
        "        print(f\"  These will be skipped during append.\")\n",
        "else:\n",
        "    # No existing table, all records are new\n",
        "    df_ground_to_append = df_ground.copy()\n",
        "    print(f\"✓ No existing table - will create new table with {len(df_ground_to_append)} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Data to BigQuery\n",
        "\n",
        "Upload new records to BigQuery using CSV format:\n",
        "\n",
        "1. **Prepare Data**: Convert all columns to strings (avoids pandas nullable integer → float conversion)\n",
        "2. **Write to CSV**: Create temporary CSV files with string data\n",
        "3. **Upload to BigQuery**: Load CSV files directly to BigQuery (BigQuery handles type parsing automatically)\n",
        "4. **Cleanup**: Remove temporary files\n",
        "\n",
        "This approach avoids PyArrow serialization issues when using `load_table_from_dataframe`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing data for CSV export...\n",
            "✓ Vegetation data prepared: 7799 rows, all columns as strings\n",
            "✓ Ground data prepared: 7799 rows, all columns as strings\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for CSV upload by converting all columns to strings\n",
        "# For float columns that should be integers, convert to int first (removing .0)\n",
        "# BigQuery will handle the type conversions based on the existing table schema\n",
        "\n",
        "print(\"Preparing data for CSV export...\")\n",
        "\n",
        "if df_vegetation_to_append is not None and len(df_vegetation_to_append) > 0:\n",
        "    df_vegetation_csv = df_vegetation_to_append.copy()\n",
        "    \n",
        "    # Integer columns that may be float64 due to NaN\n",
        "    int_cols = ['grid_point', 'year', 'intercept_1', 'intercept_2', 'intercept_3', 'intercept_4']\n",
        "    \n",
        "    # Convert float columns to int (fillna with empty string will happen in string conversion)\n",
        "    for col in int_cols:\n",
        "        if col in df_vegetation_csv.columns:\n",
        "            # For NaN values, pandas will convert them to 'nan' string, which we'll replace with ''\n",
        "            df_vegetation_csv[col] = df_vegetation_csv[col].apply(\n",
        "                lambda x: str(int(x)) if pd.notna(x) else ''\n",
        "            )\n",
        "    \n",
        "    # Convert remaining columns to strings\n",
        "    for col in df_vegetation_csv.columns:\n",
        "        if col not in int_cols:\n",
        "            df_vegetation_csv[col] = df_vegetation_csv[col].astype(str).replace('nan', '').replace('<NA>', '')\n",
        "    \n",
        "    print(f\"✓ Vegetation data prepared: {len(df_vegetation_csv)} rows, all columns as strings\")\n",
        "else:\n",
        "    df_vegetation_csv = None\n",
        "    print(\"✓ No vegetation data to prepare\")\n",
        "\n",
        "if df_ground_to_append is not None and len(df_ground_to_append) > 0:\n",
        "    df_ground_csv = df_ground_to_append.copy()\n",
        "    \n",
        "    # Integer columns that may be float64 due to NaN\n",
        "    int_cols = ['grid_point', 'year', 'intercept_1']\n",
        "    \n",
        "    # Convert float columns to int (fillna with empty string will happen in string conversion)\n",
        "    for col in int_cols:\n",
        "        if col in df_ground_csv.columns:\n",
        "            df_ground_csv[col] = df_ground_csv[col].apply(\n",
        "                lambda x: str(int(x)) if pd.notna(x) else ''\n",
        "            )\n",
        "    \n",
        "    # Convert remaining columns to strings\n",
        "    for col in df_ground_csv.columns:\n",
        "        if col not in int_cols:\n",
        "            df_ground_csv[col] = df_ground_csv[col].astype(str).replace('nan', '').replace('<NA>', '')\n",
        "    \n",
        "    print(f\"✓ Ground data prepared: {len(df_ground_csv)} rows, all columns as strings\")\n",
        "else:\n",
        "    df_ground_csv = None\n",
        "    print(\"✓ No ground data to prepare\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "UPLOADING VEGETATION DATA\n",
            "============================================================\n",
            "\n",
            "Records to upload: 7799\n",
            "Target table: mpg-data-warehouse.vegetation_point_intercept_gridVeg.gridVeg_point_intercept_vegetation\n",
            "✓ Wrote 7799 rows to temporary CSV\n",
            "Uploading to BigQuery...\n",
            "✓ Cleaned up temporary file\n"
          ]
        },
        {
          "ename": "BadRequest",
          "evalue": "400 Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 762; errors: 100. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 762; errors: 100. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 762; errors: 100; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 4 byte_offset_to_start_of_line: 249 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 9 byte_offset_to_start_of_line: 594 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 18 byte_offset_to_start_of_line: 1217 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"334.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 24 byte_offset_to_start_of_line: 1636 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 25 byte_offset_to_start_of_line: 1710 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 33 byte_offset_to_start_of_line: 2266 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"361.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 43 byte_offset_to_start_of_line: 2961 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"334.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 45 byte_offset_to_start_of_line: 3104 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"361.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 54 byte_offset_to_start_of_line: 3727 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 73 byte_offset_to_start_of_line: 5036 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"361.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 107 byte_offset_to_start_of_line: 7381 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"361.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 129 byte_offset_to_start_of_line: 8900 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"334.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 138 byte_offset_to_start_of_line: 9526 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 144 byte_offset_to_start_of_line: 9945 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 150 byte_offset_to_start_of_line: 10363 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 171 byte_offset_to_start_of_line: 11811 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"197.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 181 byte_offset_to_start_of_line: 12505 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 182 byte_offset_to_start_of_line: 12579 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 194 byte_offset_to_start_of_line: 13412 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"334.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 238 byte_offset_to_start_of_line: 16443 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 240 byte_offset_to_start_of_line: 16586 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 249 byte_offset_to_start_of_line: 17211 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 271 byte_offset_to_start_of_line: 18723 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 301 byte_offset_to_start_of_line: 20797 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 306 byte_offset_to_start_of_line: 21143 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 355 byte_offset_to_start_of_line: 24517 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 357 byte_offset_to_start_of_line: 24658 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 363 byte_offset_to_start_of_line: 25073 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 367 byte_offset_to_start_of_line: 25353 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 407 byte_offset_to_start_of_line: 28113 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 409 byte_offset_to_start_of_line: 28254 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 411 byte_offset_to_start_of_line: 28395 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 427 byte_offset_to_start_of_line: 29504 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 428 byte_offset_to_start_of_line: 29578 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 431 byte_offset_to_start_of_line: 29790 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 447 byte_offset_to_start_of_line: 30899 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 449 byte_offset_to_start_of_line: 31042 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 454 byte_offset_to_start_of_line: 31390 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 458 byte_offset_to_start_of_line: 31667 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 462 byte_offset_to_start_of_line: 31945 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"411.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 465 byte_offset_to_start_of_line: 32156 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 469 byte_offset_to_start_of_line: 32436 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"5.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 470 byte_offset_to_start_of_line: 32507 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 473 byte_offset_to_start_of_line: 32719 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 474 byte_offset_to_start_of_line: 32793 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 477 byte_offset_to_start_of_line: 33005 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"411.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 497 byte_offset_to_start_of_line: 34388 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 498 byte_offset_to_start_of_line: 34462 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 502 byte_offset_to_start_of_line: 34743 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 517 byte_offset_to_start_of_line: 35774 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 524 byte_offset_to_start_of_line: 36262 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 532 byte_offset_to_start_of_line: 36819 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 550 byte_offset_to_start_of_line: 38065 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 552 byte_offset_to_start_of_line: 38208 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 558 byte_offset_to_start_of_line: 38621 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 562 byte_offset_to_start_of_line: 38899 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 564 byte_offset_to_start_of_line: 39042 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 595 byte_offset_to_start_of_line: 41186 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 596 byte_offset_to_start_of_line: 41260 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 606 byte_offset_to_start_of_line: 41947 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 614 byte_offset_to_start_of_line: 42491 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 629 byte_offset_to_start_of_line: 43513 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 634 byte_offset_to_start_of_line: 43856 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 635 byte_offset_to_start_of_line: 43929 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 643 byte_offset_to_start_of_line: 44478 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 651 byte_offset_to_start_of_line: 45026 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 661 byte_offset_to_start_of_line: 45698 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 662 byte_offset_to_start_of_line: 45770 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 672 byte_offset_to_start_of_line: 46454 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 674 byte_offset_to_start_of_line: 46595 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 678 byte_offset_to_start_of_line: 46872 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 679 byte_offset_to_start_of_line: 46945 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 681 byte_offset_to_start_of_line: 47086 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 682 byte_offset_to_start_of_line: 47159 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 683 byte_offset_to_start_of_line: 47232 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 684 byte_offset_to_start_of_line: 47305 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 689 byte_offset_to_start_of_line: 47650 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 695 byte_offset_to_start_of_line: 48063 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 698 byte_offset_to_start_of_line: 48272 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 699 byte_offset_to_start_of_line: 48345 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 701 byte_offset_to_start_of_line: 48486 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"525.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 711 byte_offset_to_start_of_line: 49162 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"411.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 717 byte_offset_to_start_of_line: 49574 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 720 byte_offset_to_start_of_line: 49783 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"525.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 721 byte_offset_to_start_of_line: 49856 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 728 byte_offset_to_start_of_line: 50336 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 731 byte_offset_to_start_of_line: 50542 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 732 byte_offset_to_start_of_line: 50614 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 734 byte_offset_to_start_of_line: 50755 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 735 byte_offset_to_start_of_line: 50828 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 737 byte_offset_to_start_of_line: 50969 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 738 byte_offset_to_start_of_line: 51046 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 739 byte_offset_to_start_of_line: 51118 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 740 byte_offset_to_start_of_line: 51191 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 741 byte_offset_to_start_of_line: 51263 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 743 byte_offset_to_start_of_line: 51403 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 751 byte_offset_to_start_of_line: 51952 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"525.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 761 byte_offset_to_start_of_line: 52628 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 762 byte_offset_to_start_of_line: 52701 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 763 byte_offset_to_start_of_line: 52773 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 39\u001b[0m\n\u001b[1;32m     32\u001b[0m     load_job \u001b[38;5;241m=\u001b[39m bq_client\u001b[38;5;241m.\u001b[39mload_table_from_file(\n\u001b[1;32m     33\u001b[0m         source_file,\n\u001b[1;32m     34\u001b[0m         BQ_TABLE_VEGETATION,\n\u001b[1;32m     35\u001b[0m         job_config\u001b[38;5;241m=\u001b[39mjob_config\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Wait for job to complete\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mload_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Vegetation upload completed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Rows loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_job\u001b[38;5;241m.\u001b[39moutput_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3-new/envs/gcloud/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:1001\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1000\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m\"\u001b[39m: retry}\n\u001b[0;32m-> 1001\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3-new/envs/gcloud/lib/python3.9/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 762; errors: 100. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 762; errors: 100. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 762; errors: 100; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 4 byte_offset_to_start_of_line: 249 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 9 byte_offset_to_start_of_line: 594 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 18 byte_offset_to_start_of_line: 1217 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"334.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 24 byte_offset_to_start_of_line: 1636 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 25 byte_offset_to_start_of_line: 1710 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 33 byte_offset_to_start_of_line: 2266 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"361.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 43 byte_offset_to_start_of_line: 2961 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"334.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 45 byte_offset_to_start_of_line: 3104 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"361.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 54 byte_offset_to_start_of_line: 3727 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 73 byte_offset_to_start_of_line: 5036 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"361.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 107 byte_offset_to_start_of_line: 7381 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"361.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 129 byte_offset_to_start_of_line: 8900 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"334.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 138 byte_offset_to_start_of_line: 9526 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 144 byte_offset_to_start_of_line: 9945 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 150 byte_offset_to_start_of_line: 10363 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 171 byte_offset_to_start_of_line: 11811 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"197.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 181 byte_offset_to_start_of_line: 12505 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 182 byte_offset_to_start_of_line: 12579 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 194 byte_offset_to_start_of_line: 13412 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"334.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 238 byte_offset_to_start_of_line: 16443 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 240 byte_offset_to_start_of_line: 16586 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 249 byte_offset_to_start_of_line: 17211 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 271 byte_offset_to_start_of_line: 18723 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 301 byte_offset_to_start_of_line: 20797 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 306 byte_offset_to_start_of_line: 21143 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 355 byte_offset_to_start_of_line: 24517 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 357 byte_offset_to_start_of_line: 24658 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 363 byte_offset_to_start_of_line: 25073 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 367 byte_offset_to_start_of_line: 25353 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 407 byte_offset_to_start_of_line: 28113 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 409 byte_offset_to_start_of_line: 28254 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 411 byte_offset_to_start_of_line: 28395 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 427 byte_offset_to_start_of_line: 29504 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 428 byte_offset_to_start_of_line: 29578 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 431 byte_offset_to_start_of_line: 29790 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 447 byte_offset_to_start_of_line: 30899 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 449 byte_offset_to_start_of_line: 31042 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 454 byte_offset_to_start_of_line: 31390 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 458 byte_offset_to_start_of_line: 31667 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 462 byte_offset_to_start_of_line: 31945 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"411.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 465 byte_offset_to_start_of_line: 32156 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 469 byte_offset_to_start_of_line: 32436 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"5.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 470 byte_offset_to_start_of_line: 32507 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 473 byte_offset_to_start_of_line: 32719 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 474 byte_offset_to_start_of_line: 32793 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 477 byte_offset_to_start_of_line: 33005 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"411.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 497 byte_offset_to_start_of_line: 34388 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 498 byte_offset_to_start_of_line: 34462 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 502 byte_offset_to_start_of_line: 34743 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 517 byte_offset_to_start_of_line: 35774 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 524 byte_offset_to_start_of_line: 36262 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"265.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 532 byte_offset_to_start_of_line: 36819 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 550 byte_offset_to_start_of_line: 38065 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 552 byte_offset_to_start_of_line: 38208 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 558 byte_offset_to_start_of_line: 38621 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 562 byte_offset_to_start_of_line: 38899 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 564 byte_offset_to_start_of_line: 39042 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 595 byte_offset_to_start_of_line: 41186 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 596 byte_offset_to_start_of_line: 41260 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 606 byte_offset_to_start_of_line: 41947 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 614 byte_offset_to_start_of_line: 42491 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 629 byte_offset_to_start_of_line: 43513 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 634 byte_offset_to_start_of_line: 43856 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 635 byte_offset_to_start_of_line: 43929 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 643 byte_offset_to_start_of_line: 44478 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 651 byte_offset_to_start_of_line: 45026 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 661 byte_offset_to_start_of_line: 45698 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 662 byte_offset_to_start_of_line: 45770 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 672 byte_offset_to_start_of_line: 46454 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 674 byte_offset_to_start_of_line: 46595 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 678 byte_offset_to_start_of_line: 46872 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 679 byte_offset_to_start_of_line: 46945 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 681 byte_offset_to_start_of_line: 47086 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 682 byte_offset_to_start_of_line: 47159 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 683 byte_offset_to_start_of_line: 47232 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 684 byte_offset_to_start_of_line: 47305 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 689 byte_offset_to_start_of_line: 47650 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 695 byte_offset_to_start_of_line: 48063 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 698 byte_offset_to_start_of_line: 48272 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 699 byte_offset_to_start_of_line: 48345 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 701 byte_offset_to_start_of_line: 48486 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"525.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 711 byte_offset_to_start_of_line: 49162 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"411.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 717 byte_offset_to_start_of_line: 49574 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 720 byte_offset_to_start_of_line: 49783 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"525.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 721 byte_offset_to_start_of_line: 49856 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 728 byte_offset_to_start_of_line: 50336 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 731 byte_offset_to_start_of_line: 50542 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 732 byte_offset_to_start_of_line: 50614 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 734 byte_offset_to_start_of_line: 50755 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 735 byte_offset_to_start_of_line: 50828 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 737 byte_offset_to_start_of_line: 50969 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"82.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 738 byte_offset_to_start_of_line: 51046 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 739 byte_offset_to_start_of_line: 51118 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 740 byte_offset_to_start_of_line: 51191 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 741 byte_offset_to_start_of_line: 51263 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 743 byte_offset_to_start_of_line: 51403 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"405.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 751 byte_offset_to_start_of_line: 51952 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"525.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 761 byte_offset_to_start_of_line: 52628 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"529.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 762 byte_offset_to_start_of_line: 52701 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\"; reason: invalid, message: Error while reading data, error message: Unable to parse; line_number: 763 byte_offset_to_start_of_line: 52773 column_index: 7 column_name: \"intercept_2\" column_type: INT64 value: \"225.0\""
          ]
        }
      ],
      "source": [
        "# Upload VEGETATION data\n",
        "print(\"=\" * 60)\n",
        "print(\"UPLOADING VEGETATION DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if df_vegetation_csv is not None and len(df_vegetation_csv) > 0:\n",
        "    import tempfile\n",
        "    import os\n",
        "    \n",
        "    print(f\"\\nRecords to upload: {len(df_vegetation_csv)}\")\n",
        "    print(f\"Target table: {BQ_TABLE_VEGETATION}\")\n",
        "    \n",
        "    # Write to temporary CSV file (all columns as strings)\n",
        "    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, newline='') as f:\n",
        "        temp_csv_path = f.name\n",
        "        df_vegetation_csv.to_csv(f, index=False)\n",
        "    \n",
        "    print(f\"✓ Wrote {len(df_vegetation_csv)} rows to temporary CSV\")\n",
        "    \n",
        "    # Configure load job\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,  # Skip header row\n",
        "        write_disposition=\"WRITE_APPEND\",  # Append to existing table\n",
        "        autodetect=False,  # Use existing table schema\n",
        "    )\n",
        "    \n",
        "    # Upload CSV file to BigQuery\n",
        "    print(f\"Uploading to BigQuery...\")\n",
        "    try:\n",
        "        with open(temp_csv_path, 'rb') as source_file:\n",
        "            load_job = bq_client.load_table_from_file(\n",
        "                source_file,\n",
        "                BQ_TABLE_VEGETATION,\n",
        "                job_config=job_config\n",
        "            )\n",
        "        \n",
        "        # Wait for job to complete\n",
        "        load_job.result()\n",
        "        \n",
        "        print(f\"\\n✓ Vegetation upload completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"  Rows loaded: {load_job.output_rows}\")\n",
        "        print(f\"  Job ID: {load_job.job_id}\")\n",
        "        \n",
        "    finally:\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_csv_path):\n",
        "            os.unlink(temp_csv_path)\n",
        "            print(f\"✓ Cleaned up temporary file\")\n",
        "else:\n",
        "    print(\"\\nNo new vegetation records to upload.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload GROUND data\n",
        "print(\"=\" * 60)\n",
        "print(\"UPLOADING GROUND DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if df_ground_csv is not None and len(df_ground_csv) > 0:\n",
        "    import tempfile\n",
        "    import os\n",
        "    \n",
        "    print(f\"\\nRecords to upload: {len(df_ground_csv)}\")\n",
        "    print(f\"Target table: {BQ_TABLE_GROUND}\")\n",
        "    \n",
        "    # Write to temporary CSV file (all columns as strings)\n",
        "    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, newline='') as f:\n",
        "        temp_csv_path = f.name\n",
        "        df_ground_csv.to_csv(f, index=False)\n",
        "    \n",
        "    print(f\"✓ Wrote {len(df_ground_csv)} rows to temporary CSV\")\n",
        "    \n",
        "    # Configure load job\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,  # Skip header row\n",
        "        write_disposition=\"WRITE_APPEND\",  # Append to existing table\n",
        "        autodetect=False,  # Use existing table schema\n",
        "    )\n",
        "    \n",
        "    # Upload CSV file to BigQuery\n",
        "    print(f\"Uploading to BigQuery...\")\n",
        "    try:\n",
        "        with open(temp_csv_path, 'rb') as source_file:\n",
        "            load_job = bq_client.load_table_from_file(\n",
        "                source_file,\n",
        "                BQ_TABLE_GROUND,\n",
        "                job_config=job_config\n",
        "            )\n",
        "        \n",
        "        # Wait for job to complete\n",
        "        load_job.result()\n",
        "        \n",
        "        print(f\"\\n✓ Ground upload completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"  Rows loaded: {load_job.output_rows}\")\n",
        "        print(f\"  Job ID: {load_job.job_id}\")\n",
        "        \n",
        "    finally:\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_csv_path):\n",
        "            os.unlink(temp_csv_path)\n",
        "            print(f\"✓ Cleaned up temporary file\")\n",
        "else:\n",
        "    print(\"\\nNo new ground records to upload.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Uploads\n",
        "\n",
        "Query the tables to verify the data was uploaded correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify uploads by querying the tables\n",
        "print(\"=\" * 60)\n",
        "print(\"VERIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify VEGETATION table\n",
        "print(\"\\nVEGETATION Table:\")\n",
        "query_veg = f\"SELECT COUNT(*) as total_rows, COUNT(DISTINCT year) as distinct_years FROM `{BQ_TABLE_VEGETATION}`\"\n",
        "result_veg = bq_client.query(query_veg).to_dataframe()\n",
        "print(f\"  Total rows: {result_veg['total_rows'].iloc[0]}\")\n",
        "print(f\"  Distinct years: {result_veg['distinct_years'].iloc[0]}\")\n",
        "\n",
        "# Check 2025 data specifically\n",
        "query_2025_veg = f\"SELECT COUNT(*) as count_2025 FROM `{BQ_TABLE_VEGETATION}` WHERE year = 2025\"\n",
        "result_2025_veg = bq_client.query(query_2025_veg).to_dataframe()\n",
        "print(f\"  2025 rows: {result_2025_veg['count_2025'].iloc[0]}\")\n",
        "\n",
        "# Verify GROUND table\n",
        "print(\"\\nGROUND Table:\")\n",
        "query_ground = f\"SELECT COUNT(*) as total_rows, COUNT(DISTINCT year) as distinct_years FROM `{BQ_TABLE_GROUND}`\"\n",
        "result_ground = bq_client.query(query_ground).to_dataframe()\n",
        "print(f\"  Total rows: {result_ground['total_rows'].iloc[0]}\")\n",
        "print(f\"  Distinct years: {result_ground['distinct_years'].iloc[0]}\")\n",
        "\n",
        "# Check 2025 data specifically\n",
        "query_2025_ground = f\"SELECT COUNT(*) as count_2025 FROM `{BQ_TABLE_GROUND}` WHERE year = 2025\"\n",
        "result_2025_ground = bq_client.query(query_2025_ground).to_dataframe()\n",
        "print(f\"  2025 rows: {result_2025_ground['count_2025'].iloc[0]}\")\n",
        "\n",
        "# Data integrity check\n",
        "if df_vegetation_to_append is not None and len(df_vegetation_to_append) > 0:\n",
        "    expected_2025_veg = len(df_vegetation_to_append)\n",
        "    actual_2025_veg = result_2025_veg['count_2025'].iloc[0]\n",
        "    print(f\"\\n✓ Vegetation verification:\")\n",
        "    print(f\"  Expected new 2025 rows: {expected_2025_veg}\")\n",
        "    print(f\"  Actual 2025 rows in table: {actual_2025_veg}\")\n",
        "    \n",
        "if df_ground_to_append is not None and len(df_ground_to_append) > 0:\n",
        "    expected_2025_ground = len(df_ground_to_append)\n",
        "    actual_2025_ground = result_2025_ground['count_2025'].iloc[0]\n",
        "    print(f\"\\n✓ Ground verification:\")\n",
        "    print(f\"  Expected new 2025 rows: {expected_2025_ground}\")\n",
        "    print(f\"  Actual 2025 rows in table: {actual_2025_ground}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Complete summary of the upload operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"GRIDVEG POINT INTERCEPTS UPLOAD SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n📅 Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\n📂 Source:\")\n",
        "print(f\"  CSV: {GCS_CSV_URL.split('/')[-1]}\")\n",
        "print(f\"  Location: {'/'.join(GCS_CSV_URL.split('/')[:-1])}\")\n",
        "\n",
        "print(f\"\\n🎯 Target Tables:\")\n",
        "print(f\"  Vegetation: {BQ_TABLE_VEGETATION}\")\n",
        "print(f\"  Ground:     {BQ_TABLE_GROUND}\")\n",
        "print(f\"  Project:    {bq_client.project}\")\n",
        "\n",
        "print(f\"\\n📊 Data Changes:\")\n",
        "\n",
        "# Vegetation summary\n",
        "if df_vegetation_to_append is not None and len(df_vegetation_to_append) > 0:\n",
        "    print(f\"\\n  VEGETATION TABLE:\")\n",
        "    print(f\"    Rows uploaded: {len(df_vegetation_to_append)}\")\n",
        "    if df_existing_vegetation is not None:\n",
        "        print(f\"    Previous total: {len(df_existing_vegetation)}\")\n",
        "        print(f\"    New total: {len(df_existing_vegetation) + len(df_vegetation_to_append)}\")\n",
        "    \n",
        "    # Year breakdown\n",
        "    year_counts = df_vegetation_to_append['year'].value_counts().sort_index()\n",
        "    print(f\"\\n    Uploaded records by year:\")\n",
        "    for year, count in year_counts.items():\n",
        "        print(f\"      {year}: {count} records\")\n",
        "else:\n",
        "    print(f\"\\n  VEGETATION TABLE: No changes\")\n",
        "\n",
        "# Ground summary\n",
        "if df_ground_to_append is not None and len(df_ground_to_append) > 0:\n",
        "    print(f\"\\n  GROUND TABLE:\")\n",
        "    print(f\"    Rows uploaded: {len(df_ground_to_append)}\")\n",
        "    if df_existing_ground is not None:\n",
        "        print(f\"    Previous total: {len(df_existing_ground)}\")\n",
        "        print(f\"    New total: {len(df_existing_ground) + len(df_ground_to_append)}\")\n",
        "    \n",
        "    # Year breakdown\n",
        "    year_counts = df_ground_to_append['year'].value_counts().sort_index()\n",
        "    print(f\"\\n    Uploaded records by year:\")\n",
        "    for year, count in year_counts.items():\n",
        "        print(f\"      {year}: {count} records\")\n",
        "else:\n",
        "    print(f\"\\n  GROUND TABLE: No changes\")\n",
        "\n",
        "print(f\"\\n🔄 Transformations Applied:\")\n",
        "print(f\"  ✓ Renamed columns to match BigQuery schema\")\n",
        "print(f\"  ✓ Converted date format to YYYY-MM-DD\")\n",
        "print(f\"  ✓ Filtered out 2010 records\")\n",
        "print(f\"  ✓ Split data into vegetation and ground tables\")\n",
        "\n",
        "print(f\"\\n📤 Upload Method:\")\n",
        "print(f\"  ✓ CSV upload (avoids PyArrow serialization issues)\")\n",
        "print(f\"  ✓ BigQuery handles type parsing automatically\")\n",
        "\n",
        "has_changes = (df_vegetation_to_append is not None and len(df_vegetation_to_append) > 0) or \\\n",
        "              (df_ground_to_append is not None and len(df_ground_to_append) > 0)\n",
        "\n",
        "if has_changes:\n",
        "    print(f\"\\n✅ Upload completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\n✅ No changes needed - tables are up to date!\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
