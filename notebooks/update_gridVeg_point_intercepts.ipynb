{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Update gridVeg Point Intercepts in BigQuery\n",
        "\n",
        "This notebook appends new point intercept data to BigQuery tables from a CSV file stored in GCS.\n",
        "\n",
        "**Operation**: APPEND new rows (not replace entire table)\n",
        "\n",
        "**Target Tables**:\n",
        "- `gridVeg_point_intercept_vegetation` - vegetation intercept data (4 height layers)\n",
        "- `gridVeg_point_intercept_ground` - ground cover intercept data\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, google-cloud-storage, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values for gridVeg point intercepts\n",
        "GCS_CSV_URL = config['gridveg_point_intercepts']['gcs']['csv_url']\n",
        "BACKUP_BUCKET = config['gridveg_point_intercepts']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gridveg_point_intercepts']['gcs'].get('backup_prefix', 'backups/gridveg_point_intercepts')\n",
        "\n",
        "BQ_TABLE_VEGETATION = config['gridveg_point_intercepts']['bigquery']['table_vegetation']\n",
        "BQ_TABLE_GROUND = config['gridveg_point_intercepts']['bigquery']['table_ground']\n",
        "BQ_PROJECT = config['gridveg_point_intercepts']['bigquery'].get('project')\n",
        "\n",
        "# Verify required config values\n",
        "if not GCS_CSV_URL or GCS_CSV_URL.startswith('gs://your-'):\n",
        "    raise ValueError(\"Please configure gridveg_point_intercepts.gcs.csv_url in config.yml\")\n",
        "if not BQ_TABLE_VEGETATION or 'your-project' in BQ_TABLE_VEGETATION:\n",
        "    raise ValueError(\"Please configure gridveg_point_intercepts.bigquery.table_vegetation in config.yml\")\n",
        "if not BQ_TABLE_GROUND or 'your-project' in BQ_TABLE_GROUND:\n",
        "    raise ValueError(\"Please configure gridveg_point_intercepts.bigquery.table_ground in config.yml\")\n",
        "\n",
        "print(\"✓ Configuration loaded successfully\")\n",
        "print(f\"  CSV URL: {GCS_CSV_URL[:60]}...\" if len(GCS_CSV_URL) > 60 else f\"  CSV URL: {GCS_CSV_URL}\")\n",
        "print(f\"  Vegetation Table: {BQ_TABLE_VEGETATION}\")\n",
        "print(f\"  Ground Table: {BQ_TABLE_GROUND}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "storage_client = storage.Client(project=BQ_PROJECT) if BQ_PROJECT else storage.Client()\n",
        "\n",
        "print(f\"✓ Clients initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV from GCS (new data)\n",
        "print(\"Reading CSV from GCS...\")\n",
        "df_new = pd.read_csv(GCS_CSV_URL)\n",
        "\n",
        "print(f\"✓ CSV loaded successfully:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {list(df_new.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_new.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform CSV Data\n",
        "\n",
        "The source CSV will be transformed into two separate datasets:\n",
        "\n",
        "### Vegetation Table Transformations\n",
        "- Rename columns to match BigQuery schema\n",
        "- Convert date format from mm/dd/yy to ISO format (YYYY-MM-DD)\n",
        "- Filter out 2010 records (per requirements)\n",
        "- Select columns: survey_ID, grid_point, date, year, transect_point, height_intercept_1, intercept_1-4\n",
        "\n",
        "### Ground Table Transformations\n",
        "- Rename columns to match BigQuery schema  \n",
        "- Convert date format from mm/dd/yy to ISO format (YYYY-MM-DD)\n",
        "- Filter out 2010 records (per requirements)\n",
        "- Select columns: survey_ID, grid_point, date, year, transect_point, intercept_1, intercept_ground_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column mapping for VEGETATION table\n",
        "column_mapping_vegetation = {\n",
        "    'Survey Data::__kp_Survey': 'survey_ID',\n",
        "    'Survey Data::_kf_Site': 'grid_point',\n",
        "    'Survey Data::SurveyDate': 'date',\n",
        "    'Survey Data::SurveyYear': 'year',\n",
        "    'PointTrans': 'transect_point',\n",
        "    'Height': 'height_intercept_1',\n",
        "    '_kf_Hit1_serial': 'intercept_1',\n",
        "    '_kf_Hit2_serial': 'intercept_2',\n",
        "    '_kf_Hit3_serial': 'intercept_3',\n",
        "    '_kf_Hit4_serial': 'intercept_4'\n",
        "}\n",
        "\n",
        "print(\"Vegetation table column mapping:\")\n",
        "for csv_col, bq_col in column_mapping_vegetation.items():\n",
        "    print(f\"  {csv_col:30s} → {bq_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column mapping for GROUND table\n",
        "column_mapping_ground = {\n",
        "    'Survey Data::__kp_Survey': 'survey_ID',\n",
        "    'Survey Data::_kf_Site': 'grid_point',\n",
        "    'Survey Data::SurveyDate': 'date',\n",
        "    'Survey Data::SurveyYear': 'year',\n",
        "    'PointTrans': 'transect_point',\n",
        "    '_kf_Hit1_serial': 'intercept_1',\n",
        "    'GroundCover': 'intercept_ground_code'\n",
        "}\n",
        "\n",
        "print(\"Ground table column mapping:\")\n",
        "for csv_col, bq_col in column_mapping_ground.items():\n",
        "    print(f\"  {csv_col:30s} → {bq_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform data for VEGETATION table\n",
        "print(\"=\" * 60)\n",
        "print(\"TRANSFORMING DATA FOR VEGETATION TABLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Select and rename columns\n",
        "df_vegetation = df_new[list(column_mapping_vegetation.keys())].copy()\n",
        "df_vegetation = df_vegetation.rename(columns=column_mapping_vegetation)\n",
        "\n",
        "print(f\"\\n✓ Columns renamed\")\n",
        "print(f\"  Transformed columns: {list(df_vegetation.columns)}\")\n",
        "\n",
        "# Convert date from m/d/yy to proper datetime/date format\n",
        "df_vegetation['date'] = pd.to_datetime(df_vegetation['date'], format='%m/%d/%y').dt.date\n",
        "\n",
        "print(f\"✓ Date format converted to date type\")\n",
        "print(f\"  Sample dates: {df_vegetation['date'].head(3).tolist()}\")\n",
        "\n",
        "# Filter out 2010 records\n",
        "rows_before = len(df_vegetation)\n",
        "df_vegetation = df_vegetation[df_vegetation['year'] != 2010].copy()\n",
        "rows_after = len(df_vegetation)\n",
        "\n",
        "print(f\"\\n✓ Filtered out 2010 records\")\n",
        "print(f\"  Rows before: {rows_before}\")\n",
        "print(f\"  Rows after:  {rows_after}\")\n",
        "print(f\"  Removed:     {rows_before - rows_after}\")\n",
        "\n",
        "print(f\"\\nVegetation data preview:\")\n",
        "df_vegetation.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform data for GROUND table\n",
        "print(\"=\" * 60)\n",
        "print(\"TRANSFORMING DATA FOR GROUND TABLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Select and rename columns\n",
        "df_ground = df_new[list(column_mapping_ground.keys())].copy()\n",
        "df_ground = df_ground.rename(columns=column_mapping_ground)\n",
        "\n",
        "print(f\"\\n✓ Columns renamed\")\n",
        "print(f\"  Transformed columns: {list(df_ground.columns)}\")\n",
        "\n",
        "# Convert date from m/d/yy to proper datetime/date format\n",
        "df_ground['date'] = pd.to_datetime(df_ground['date'], format='%m/%d/%y').dt.date\n",
        "\n",
        "print(f\"✓ Date format converted to date type\")\n",
        "print(f\"  Sample dates: {df_ground['date'].head(3).tolist()}\")\n",
        "\n",
        "# Filter out 2010 records\n",
        "rows_before = len(df_ground)\n",
        "df_ground = df_ground[df_ground['year'] != 2010].copy()\n",
        "rows_after = len(df_ground)\n",
        "\n",
        "print(f\"\\n✓ Filtered out 2010 records\")\n",
        "print(f\"  Rows before: {rows_before}\")\n",
        "print(f\"  Rows after:  {rows_after}\")\n",
        "print(f\"  Removed:     {rows_before - rows_after}\")\n",
        "\n",
        "print(f\"\\nGround data preview:\")\n",
        "df_ground.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Existing BigQuery Tables\n",
        "\n",
        "Load the current data from both BigQuery tables to compare with the new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read existing VEGETATION table from BigQuery\n",
        "print(f\"Reading existing VEGETATION data from {BQ_TABLE_VEGETATION}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_VEGETATION}`\"\n",
        "\n",
        "try:\n",
        "    df_existing_vegetation = bq_client.query(query).to_dataframe()\n",
        "    print(f\"✓ Existing vegetation table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing_vegetation)}\")\n",
        "    print(f\"  Columns: {list(df_existing_vegetation.columns)}\")\n",
        "    print(f\"\\nExisting vegetation data preview:\")\n",
        "    display(df_existing_vegetation.head())\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error reading table: {e}\")\n",
        "    print(\"  This may be expected if the table doesn't exist yet.\")\n",
        "    df_existing_vegetation = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read existing GROUND table from BigQuery\n",
        "print(f\"Reading existing GROUND data from {BQ_TABLE_GROUND}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_GROUND}`\"\n",
        "\n",
        "try:\n",
        "    df_existing_ground = bq_client.query(query).to_dataframe()\n",
        "    print(f\"✓ Existing ground table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing_ground)}\")\n",
        "    print(f\"  Columns: {list(df_existing_ground.columns)}\")\n",
        "    print(f\"\\nExisting ground data preview:\")\n",
        "    display(df_existing_ground.head())\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error reading table: {e}\")\n",
        "    print(\"  This may be expected if the table doesn't exist yet.\")\n",
        "    df_existing_ground = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare New vs Existing Data\n",
        "\n",
        "Identify which rows in the new data are not already in the existing tables.\n",
        "\n",
        "**Unique identifier**: `survey_ID + transect_point` (each survey has multiple transect points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare VEGETATION datasets\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARING VEGETATION DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if df_existing_vegetation is not None:\n",
        "    print(f\"\\nRow count:\")\n",
        "    print(f\"  Existing: {len(df_existing_vegetation)}\")\n",
        "    print(f\"  New CSV:  {len(df_vegetation)}\")\n",
        "    \n",
        "    # Column comparison\n",
        "    existing_cols = set(df_existing_vegetation.columns)\n",
        "    new_cols = set(df_vegetation.columns)\n",
        "    \n",
        "    if existing_cols == new_cols:\n",
        "        print(f\"\\n✓ Columns match ({len(new_cols)} columns)\")\n",
        "    else:\n",
        "        print(\"\\n⚠ Column differences detected:\")\n",
        "        if new_cols - existing_cols:\n",
        "            print(f\"  New columns: {new_cols - existing_cols}\")\n",
        "        if existing_cols - new_cols:\n",
        "            print(f\"  Missing columns: {existing_cols - new_cols}\")\n",
        "    \n",
        "    # Create composite key for comparison\n",
        "    df_existing_vegetation['_composite_key'] = (\n",
        "        df_existing_vegetation['survey_ID'].astype(str) + '|' + \n",
        "        df_existing_vegetation['transect_point'].astype(str)\n",
        "    )\n",
        "    df_vegetation['_composite_key'] = (\n",
        "        df_vegetation['survey_ID'].astype(str) + '|' + \n",
        "        df_vegetation['transect_point'].astype(str)\n",
        "    )\n",
        "    \n",
        "    existing_keys = set(df_existing_vegetation['_composite_key'])\n",
        "    new_keys = set(df_vegetation['_composite_key'])\n",
        "    \n",
        "    # Find records to append\n",
        "    keys_to_append = new_keys - existing_keys\n",
        "    \n",
        "    if keys_to_append:\n",
        "        df_vegetation_to_append = df_vegetation[df_vegetation['_composite_key'].isin(keys_to_append)].copy()\n",
        "        # Drop the temporary composite key\n",
        "        df_vegetation_to_append = df_vegetation_to_append.drop(columns=['_composite_key'])\n",
        "        \n",
        "        print(f\"\\n✓ Found {len(df_vegetation_to_append)} new vegetation records to append\")\n",
        "        \n",
        "        # Show year breakdown\n",
        "        print(f\"\\nNew records by year:\")\n",
        "        year_counts = df_vegetation_to_append['year'].value_counts().sort_index()\n",
        "        for year, count in year_counts.items():\n",
        "            print(f\"  {year}: {count} records\")\n",
        "    else:\n",
        "        df_vegetation_to_append = None\n",
        "        print(\"\\n⚠ No new records found - all keys already exist in table\")\n",
        "    \n",
        "    # Check for duplicates\n",
        "    duplicate_keys = existing_keys & new_keys\n",
        "    if duplicate_keys:\n",
        "        print(f\"\\n⚠ Warning: {len(duplicate_keys)} records already exist in table\")\n",
        "        print(f\"  These will be skipped during append.\")\n",
        "else:\n",
        "    # No existing table, all records are new\n",
        "    df_vegetation_to_append = df_vegetation.copy()\n",
        "    print(f\"✓ No existing table - will create new table with {len(df_vegetation_to_append)} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare GROUND datasets\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARING GROUND DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if df_existing_ground is not None:\n",
        "    print(f\"\\nRow count:\")\n",
        "    print(f\"  Existing: {len(df_existing_ground)}\")\n",
        "    print(f\"  New CSV:  {len(df_ground)}\")\n",
        "    \n",
        "    # Column comparison\n",
        "    existing_cols = set(df_existing_ground.columns)\n",
        "    new_cols = set(df_ground.columns)\n",
        "    \n",
        "    if existing_cols == new_cols:\n",
        "        print(f\"\\n✓ Columns match ({len(new_cols)} columns)\")\n",
        "    else:\n",
        "        print(\"\\n⚠ Column differences detected:\")\n",
        "        if new_cols - existing_cols:\n",
        "            print(f\"  New columns: {new_cols - existing_cols}\")\n",
        "        if existing_cols - new_cols:\n",
        "            print(f\"  Missing columns: {existing_cols - new_cols}\")\n",
        "    \n",
        "    # Create composite key for comparison\n",
        "    df_existing_ground['_composite_key'] = (\n",
        "        df_existing_ground['survey_ID'].astype(str) + '|' + \n",
        "        df_existing_ground['transect_point'].astype(str)\n",
        "    )\n",
        "    df_ground['_composite_key'] = (\n",
        "        df_ground['survey_ID'].astype(str) + '|' + \n",
        "        df_ground['transect_point'].astype(str)\n",
        "    )\n",
        "    \n",
        "    existing_keys = set(df_existing_ground['_composite_key'])\n",
        "    new_keys = set(df_ground['_composite_key'])\n",
        "    \n",
        "    # Find records to append\n",
        "    keys_to_append = new_keys - existing_keys\n",
        "    \n",
        "    if keys_to_append:\n",
        "        df_ground_to_append = df_ground[df_ground['_composite_key'].isin(keys_to_append)].copy()\n",
        "        # Drop the temporary composite key\n",
        "        df_ground_to_append = df_ground_to_append.drop(columns=['_composite_key'])\n",
        "        \n",
        "        print(f\"\\n✓ Found {len(df_ground_to_append)} new ground records to append\")\n",
        "        \n",
        "        # Show year breakdown\n",
        "        print(f\"\\nNew records by year:\")\n",
        "        year_counts = df_ground_to_append['year'].value_counts().sort_index()\n",
        "        for year, count in year_counts.items():\n",
        "            print(f\"  {year}: {count} records\")\n",
        "    else:\n",
        "        df_ground_to_append = None\n",
        "        print(\"\\n⚠ No new records found - all keys already exist in table\")\n",
        "    \n",
        "    # Check for duplicates\n",
        "    duplicate_keys = existing_keys & new_keys\n",
        "    if duplicate_keys:\n",
        "        print(f\"\\n⚠ Warning: {len(duplicate_keys)} records already exist in table\")\n",
        "        print(f\"  These will be skipped during append.\")\n",
        "else:\n",
        "    # No existing table, all records are new\n",
        "    df_ground_to_append = df_ground.copy()\n",
        "    print(f\"✓ No existing table - will create new table with {len(df_ground_to_append)} records\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
