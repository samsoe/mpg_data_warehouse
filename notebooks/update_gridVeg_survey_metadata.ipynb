{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Update gridVeg Survey Metadata in BigQuery\n",
        "\n",
        "This notebook appends new survey metadata to the BigQuery table from a CSV file stored in GCS.\n",
        "\n",
        "**Operation**: APPEND new rows (not replace entire table)\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, google-cloud-storage, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values for gridVeg survey metadata\n",
        "GCS_CSV_URL = config['gridveg_survey_metadata']['gcs']['csv_url']\n",
        "BACKUP_BUCKET = config['gridveg_survey_metadata']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gridveg_survey_metadata']['gcs'].get('backup_prefix', 'backups/gridveg_survey_metadata')\n",
        "BQ_TABLE_ID = config['gridveg_survey_metadata']['bigquery']['table_id']\n",
        "BQ_PROJECT = config['gridveg_survey_metadata']['bigquery'].get('project')\n",
        "\n",
        "# Verify required config values\n",
        "if not GCS_CSV_URL or GCS_CSV_URL.startswith('gs://your-'):\n",
        "    raise ValueError(\"Please configure gridveg_survey_metadata.gcs.csv_url in config.yml\")\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure gridveg_survey_metadata.bigquery.table_id in config.yml\")\n",
        "\n",
        "print(\"✓ Configuration loaded successfully\")\n",
        "print(f\"  CSV URL: {GCS_CSV_URL[:60]}...\" if len(GCS_CSV_URL) > 60 else f\"  CSV URL: {GCS_CSV_URL}\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Clients initialized\n",
            "  Project: mpg-data-warehouse\n"
          ]
        }
      ],
      "source": [
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "storage_client = storage.Client(project=BQ_PROJECT) if BQ_PROJECT else storage.Client()\n",
        "\n",
        "print(f\"✓ Clients initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading CSV from GCS...\n",
            "✓ CSV loaded successfully:\n",
            "  Rows: 39\n",
            "  Columns: ['__kp_Survey', '_kf_Site', 'SurveyYear', 'SurveyDate', 'Surveyor1']\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>__kp_Survey</th>\n",
              "      <th>_kf_Site</th>\n",
              "      <th>SurveyYear</th>\n",
              "      <th>SurveyDate</th>\n",
              "      <th>Surveyor1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B45700C5-D391-4679-8579-217DCB1385A2</td>\n",
              "      <td>227</td>\n",
              "      <td>2025</td>\n",
              "      <td>5/21/25</td>\n",
              "      <td>MLS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C0BD2A75-FF0B-48DC-BB9D-941267BF5838</td>\n",
              "      <td>190</td>\n",
              "      <td>2025</td>\n",
              "      <td>5/21/25</td>\n",
              "      <td>MLS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38A8FE64-8769-474C-BC25-01CBF006BFCC</td>\n",
              "      <td>331</td>\n",
              "      <td>2025</td>\n",
              "      <td>5/22/25</td>\n",
              "      <td>MLS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>147224CA-F0FC-4E02-B2DE-8B17F5553B29</td>\n",
              "      <td>45</td>\n",
              "      <td>2025</td>\n",
              "      <td>5/26/25</td>\n",
              "      <td>MLS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CD7E5294-F7D8-4CD6-B35A-EDB356A88A73</td>\n",
              "      <td>165</td>\n",
              "      <td>2025</td>\n",
              "      <td>5/26/25</td>\n",
              "      <td>MLS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            __kp_Survey  _kf_Site  SurveyYear SurveyDate  \\\n",
              "0  B45700C5-D391-4679-8579-217DCB1385A2       227        2025    5/21/25   \n",
              "1  C0BD2A75-FF0B-48DC-BB9D-941267BF5838       190        2025    5/21/25   \n",
              "2  38A8FE64-8769-474C-BC25-01CBF006BFCC       331        2025    5/22/25   \n",
              "3  147224CA-F0FC-4E02-B2DE-8B17F5553B29        45        2025    5/26/25   \n",
              "4  CD7E5294-F7D8-4CD6-B35A-EDB356A88A73       165        2025    5/26/25   \n",
              "\n",
              "  Surveyor1  \n",
              "0       MLS  \n",
              "1       MLS  \n",
              "2       MLS  \n",
              "3       MLS  \n",
              "4       MLS  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read CSV from GCS (new data)\n",
        "print(\"Reading CSV from GCS...\")\n",
        "df_new = pd.read_csv(GCS_CSV_URL)\n",
        "\n",
        "print(f\"✓ CSV loaded successfully:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {list(df_new.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_new.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform CSV Data\n",
        "\n",
        "Apply schema transformations to match BigQuery table:\n",
        "- Rename columns to match destination schema\n",
        "- Convert date format from mm/dd/yyyy to ISO format (YYYY-MM-DD)\n",
        "- Create `survey_sequence` variable from `year` (2011 and 2012 → \"2011-12\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column mapping:\n",
            "  __kp_Survey          → survey_ID\n",
            "  _kf_Site             → grid_point\n",
            "  SurveyYear           → year\n",
            "  SurveyDate           → date\n",
            "  Surveyor1            → surveyor\n"
          ]
        }
      ],
      "source": [
        "# Define column mapping from CSV to BigQuery\n",
        "column_mapping = {\n",
        "    '__kp_Survey': 'survey_ID',\n",
        "    '_kf_Site': 'grid_point',\n",
        "    'SurveyYear': 'year',\n",
        "    'SurveyDate': 'date',\n",
        "    'Surveyor1': 'surveyor'\n",
        "}\n",
        "\n",
        "print(\"Column mapping:\")\n",
        "for csv_col, bq_col in column_mapping.items():\n",
        "    print(f\"  {csv_col:20s} → {bq_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ CSV columns match expected schema\n",
            "\n",
            "CSV columns: ['__kp_Survey', '_kf_Site', 'SurveyYear', 'SurveyDate', 'Surveyor1']\n"
          ]
        }
      ],
      "source": [
        "# Verify CSV columns match expected schema\n",
        "expected_csv_columns = set(column_mapping.keys())\n",
        "actual_csv_columns = set(df_new.columns)\n",
        "\n",
        "if actual_csv_columns == expected_csv_columns:\n",
        "    print(\"✓ CSV columns match expected schema\")\n",
        "else:\n",
        "    print(\"⚠ CSV column differences detected:\")\n",
        "    if actual_csv_columns - expected_csv_columns:\n",
        "        print(f\"  Unexpected columns: {actual_csv_columns - expected_csv_columns}\")\n",
        "    if expected_csv_columns - actual_csv_columns:\n",
        "        print(f\"  Missing columns: {expected_csv_columns - actual_csv_columns}\")\n",
        "    \n",
        "print(f\"\\nCSV columns: {list(df_new.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Columns renamed\n",
            "  Transformed columns: ['survey_ID', 'grid_point', 'year', 'date', 'surveyor']\n"
          ]
        }
      ],
      "source": [
        "# Apply transformation: rename columns\n",
        "df_transformed = df_new.copy()\n",
        "df_transformed = df_transformed.rename(columns=column_mapping)\n",
        "\n",
        "print(\"✓ Columns renamed\")\n",
        "print(f\"  Transformed columns: {list(df_transformed.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Date format converted to ISO (YYYY-MM-DD)\n",
            "  Sample dates: ['2025-05-21', '2025-05-21', '2025-05-22', '2025-05-26', '2025-05-26']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z3/ltkv9wnd685110jpnw7gm7x00000gp/T/ipykernel_74467/1650866954.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_transformed['date'] = pd.to_datetime(df_transformed['date']).dt.strftime('%Y-%m-%d')\n"
          ]
        }
      ],
      "source": [
        "# Convert date from mm/dd/yyyy to ISO format (YYYY-MM-DD)\n",
        "# Explicitly specify format to avoid parsing warnings and ensure consistency\n",
        "df_transformed['date'] = pd.to_datetime(df_transformed['date'], format='%m/%d/%Y').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "print(\"✓ Date format converted to ISO (YYYY-MM-DD)\")\n",
        "print(f\"  Sample dates: {df_transformed['date'].head().tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create survey_sequence variable from year\n",
        "# Recode 2011 and 2012 to \"2011-12\", leave all other years as strings\n",
        "def create_survey_sequence(year):\n",
        "    if year in [2011, 2012]:\n",
        "        return \"2011-12\"\n",
        "    else:\n",
        "        return str(year)\n",
        "\n",
        "df_transformed['survey_sequence'] = df_transformed['year'].apply(create_survey_sequence)\n",
        "\n",
        "print(\"✓ Created survey_sequence variable\")\n",
        "print(f\"\\nSurvey sequence mapping:\")\n",
        "for year in sorted(df_transformed['year'].unique()):\n",
        "    seq = df_transformed[df_transformed['year'] == year]['survey_sequence'].iloc[0]\n",
        "    count = len(df_transformed[df_transformed['year'] == year])\n",
        "    print(f\"  Year {year} → '{seq}' ({count} records)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display transformed data info\n",
        "print(\"Transformed Data Info:\")\n",
        "df_transformed.info()\n",
        "print(f\"\\nTransformed data preview:\")\n",
        "df_transformed.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Existing BigQuery Table\n",
        "\n",
        "Load the current data from BigQuery to compare with the new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read existing data from BigQuery\n",
        "print(f\"Reading existing data from {BQ_TABLE_ID}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "\n",
        "try:\n",
        "    df_existing = bq_client.query(query).to_dataframe()\n",
        "    print(f\"✓ Existing table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing)}\")\n",
        "    print(f\"  Columns: {list(df_existing.columns)}\")\n",
        "    print(f\"\\nExisting data preview:\")\n",
        "    display(df_existing.head())\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error reading table: {e}\")\n",
        "    print(\"  This may be expected if the table doesn't exist yet.\")\n",
        "    df_existing = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display existing data info (if available)\n",
        "if df_existing is not None:\n",
        "    print(\"Existing Data Info:\")\n",
        "    df_existing.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare New vs Existing Data\n",
        "\n",
        "Identify which rows in the new data are not already in the existing table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare datasets\n",
        "if df_existing is not None:\n",
        "    print(\"=== Comparison Summary ===\\n\")\n",
        "    \n",
        "    # Row count comparison\n",
        "    print(f\"Row count:\")\n",
        "    print(f\"  Existing: {len(df_existing)}\")\n",
        "    print(f\"  New CSV:  {len(df_transformed)}\")\n",
        "    \n",
        "    # Column comparison\n",
        "    existing_cols = set(df_existing.columns)\n",
        "    new_cols = set(df_transformed.columns)\n",
        "    \n",
        "    if existing_cols == new_cols:\n",
        "        print(f\"\\n✓ Columns match ({len(new_cols)} columns)\")\n",
        "    else:\n",
        "        print(\"\\n⚠ Column differences detected:\")\n",
        "        if new_cols - existing_cols:\n",
        "            print(f\"  New columns: {new_cols - existing_cols}\")\n",
        "        if existing_cols - new_cols:\n",
        "            print(f\"  Missing columns: {existing_cols - new_cols}\")\n",
        "    \n",
        "    print(f\"\\nColumns: {list(df_transformed.columns)}\")\n",
        "else:\n",
        "    print(\"No existing data to compare - this will be a new table creation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify new records (not in existing table)\n",
        "# Use survey_ID as the unique identifier\n",
        "if df_existing is not None:\n",
        "    existing_ids = set(df_existing['survey_ID'])\n",
        "    new_ids = set(df_transformed['survey_ID'])\n",
        "    \n",
        "    # Find records in new data that aren't in existing\n",
        "    ids_to_append = new_ids - existing_ids\n",
        "    \n",
        "    if ids_to_append:\n",
        "        df_to_append = df_transformed[df_transformed['survey_ID'].isin(ids_to_append)].copy()\n",
        "        print(f\"✓ Found {len(df_to_append)} new records to append\")\n",
        "        \n",
        "        # Show year breakdown of new records\n",
        "        print(f\"\\nNew records by year:\")\n",
        "        year_counts = df_to_append['year'].value_counts().sort_index()\n",
        "        for year, count in year_counts.items():\n",
        "            print(f\"  {year}: {count} records\")\n",
        "        \n",
        "        print(f\"\\nSample of new records:\")\n",
        "        display(df_to_append.head())\n",
        "    else:\n",
        "        df_to_append = None\n",
        "        print(\"⚠ No new records found - all survey_IDs already exist in table\")\n",
        "        print(\"  Nothing to append.\")\n",
        "    \n",
        "    # Check for any duplicates\n",
        "    duplicate_ids = existing_ids & new_ids\n",
        "    if duplicate_ids:\n",
        "        print(f\"\\n⚠ Warning: {len(duplicate_ids)} survey_IDs already exist in table\")\n",
        "        print(f\"  These will be skipped during append.\")\n",
        "        if len(duplicate_ids) <= 10:\n",
        "            print(f\"  Duplicate IDs: {list(duplicate_ids)[:10]}\")\n",
        "else:\n",
        "    # No existing table, so all records are new\n",
        "    df_to_append = df_transformed.copy()\n",
        "    print(f\"✓ No existing table - will create new table with {len(df_to_append)} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backup Existing Table\n",
        "\n",
        "Before making any changes, create a backup of the existing table to GCS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backup existing table to GCS\n",
        "if df_existing is not None and BACKUP_BUCKET and df_to_append is not None:\n",
        "    # Generate backup path with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = f\"gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/{timestamp}/*.csv\"\n",
        "    \n",
        "    print(f\"Creating backup of existing table...\")\n",
        "    print(f\"  Destination: {backup_path}\")\n",
        "    \n",
        "    # Export table to GCS\n",
        "    extract_job = bq_client.extract_table(\n",
        "        BQ_TABLE_ID,\n",
        "        backup_path,\n",
        "        location=\"US\"\n",
        "    )\n",
        "    \n",
        "    extract_job.result()  # Wait for job to complete\n",
        "    \n",
        "    print(f\"✓ Backup completed successfully\")\n",
        "    print(f\"  Files: {backup_path}\")\n",
        "elif df_existing is None:\n",
        "    print(\"⚠ No existing table to backup (table doesn't exist yet)\")\n",
        "elif not BACKUP_BUCKET:\n",
        "    print(\"⚠ Backup bucket not configured in config.yml\")\n",
        "    print(\"  Set 'gridveg_survey_metadata.gcs.backup_bucket' to enable automatic backups\")\n",
        "elif df_to_append is None:\n",
        "    print(\"⚠ No new records to append, skipping backup\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Append New Records to BigQuery\n",
        "\n",
        "⚠️ **IMPORTANT**: This will APPEND new rows to the existing table (not replace).\n",
        "\n",
        "Review the comparison above before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Append new records to BigQuery\n",
        "if df_to_append is not None and len(df_to_append) > 0:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"APPENDING TO BIGQUERY TABLE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nTable: {BQ_TABLE_ID}\")\n",
        "    print(f\"Rows to append: {len(df_to_append)}\")\n",
        "    print(f\"Mode: WRITE_APPEND (add to existing table)\")\n",
        "    print(f\"\\nStarting append at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "    \n",
        "    # Configure job to append to existing table\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=\"WRITE_APPEND\"  # Append to existing table\n",
        "    )\n",
        "    \n",
        "    # Load dataframe to BigQuery\n",
        "    load_job = bq_client.load_table_from_dataframe(\n",
        "        df_to_append,\n",
        "        BQ_TABLE_ID,\n",
        "        job_config=job_config\n",
        "    )\n",
        "    \n",
        "    # Wait for job to complete\n",
        "    load_job.result()\n",
        "    \n",
        "    print(f\"\\n✓ Append completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"  Rows appended: {load_job.output_rows}\")\n",
        "    print(f\"  Job ID: {load_job.job_id}\")\n",
        "else:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"NO RECORDS TO APPEND\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nNo new records found or no records to append.\")\n",
        "    print(\"Table remains unchanged.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Append\n",
        "\n",
        "Read back the table to verify the append was successful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read updated table\n",
        "print(\"Verifying append...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "df_updated = bq_client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"\\n✓ Verification complete\")\n",
        "print(f\"  Rows in table: {len(df_updated)}\")\n",
        "print(f\"  Columns: {list(df_updated.columns)}\")\n",
        "\n",
        "# Show records by year\n",
        "print(f\"\\nRecords by year:\")\n",
        "year_counts = df_updated['year'].value_counts().sort_index()\n",
        "for year, count in year_counts.items():\n",
        "    print(f\"  {year}: {count} records\")\n",
        "\n",
        "print(f\"\\nUpdated table preview:\")\n",
        "df_updated.tail(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify row counts\n",
        "if df_to_append is not None and len(df_to_append) > 0:\n",
        "    expected_rows = len(df_existing) + len(df_to_append) if df_existing is not None else len(df_to_append)\n",
        "    actual_rows = len(df_updated)\n",
        "    \n",
        "    print(\"Data integrity check:\")\n",
        "    if df_existing is not None:\n",
        "        print(f\"  Previous rows:   {len(df_existing)}\")\n",
        "        print(f\"  Rows appended:   {len(df_to_append)}\")\n",
        "        print(f\"  Expected total:  {expected_rows}\")\n",
        "        print(f\"  Actual total:    {actual_rows}\")\n",
        "    else:\n",
        "        print(f\"  Rows written:    {len(df_to_append)}\")\n",
        "        print(f\"  Rows in table:   {actual_rows}\")\n",
        "    \n",
        "    if expected_rows == actual_rows:\n",
        "        print(f\"\\n✓ Row count verified - all {len(df_to_append)} new rows successfully appended\")\n",
        "    else:\n",
        "        print(f\"\\n⚠ Row count mismatch!\")\n",
        "        print(f\"  Expected: {expected_rows}\")\n",
        "        print(f\"  Actual:   {actual_rows}\")\n",
        "        print(f\"  Difference: {actual_rows - expected_rows}\")\n",
        "else:\n",
        "    print(\"No new records were appended.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report\n",
        "\n",
        "Complete summary of the append operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"GRIDVEG SURVEY METADATA APPEND SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n📅 Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\n📂 Source:\")\n",
        "print(f\"  CSV: {GCS_CSV_URL.split('/')[-1]}\")\n",
        "print(f\"  Location: {'/'.join(GCS_CSV_URL.split('/')[:-1])}\")\n",
        "\n",
        "print(f\"\\n🎯 Target:\")\n",
        "print(f\"  Table: {BQ_TABLE_ID}\")\n",
        "print(f\"  Project: {bq_client.project}\")\n",
        "\n",
        "print(f\"\\n📊 Data Changes:\")\n",
        "if df_existing is not None:\n",
        "    print(f\"  Previous rows: {len(df_existing)}\")\n",
        "    print(f\"  New rows:      {len(df_updated)}\")\n",
        "    print(f\"  Rows appended: {len(df_updated) - len(df_existing):+d}\")\n",
        "    \n",
        "    if df_to_append is not None and len(df_to_append) > 0:\n",
        "        print(f\"\\n  Appended records by year:\")\n",
        "        year_counts = df_to_append['year'].value_counts().sort_index()\n",
        "        for year, count in year_counts.items():\n",
        "            print(f\"    {year}: {count} records\")\n",
        "else:\n",
        "    print(f\"  New table created with {len(df_updated)} rows\")\n",
        "\n",
        "print(f\"\\n🔄 Transformations Applied:\")\n",
        "print(f\"  ✓ Renamed {len(column_mapping)} columns to match BigQuery schema\")\n",
        "print(f\"  ✓ Converted date format to ISO (YYYY-MM-DD)\")\n",
        "print(f\"  ✓ Created survey_sequence variable (2011/2012 → 2011-12)\")\n",
        "\n",
        "if BACKUP_BUCKET and df_existing is not None and df_to_append is not None and len(df_to_append) > 0:\n",
        "    print(f\"\\n💾 Backup:\")\n",
        "    print(f\"  Location: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}/\")\n",
        "    print(f\"  Status: ✓ Created before append\")\n",
        "\n",
        "if df_to_append is not None and len(df_to_append) > 0:\n",
        "    print(f\"\\n✅ Append completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\n✅ No changes needed - table is up to date!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollback Instructions (If Needed)\n",
        "\n",
        "If you need to rollback to the previous version, use the backup created at the beginning of this notebook.\n",
        "\n",
        "```python\n",
        "# To rollback, first delete the appended rows:\n",
        "# df_rollback = df_updated[~df_updated['survey_ID'].isin(df_to_append['survey_ID'])]\n",
        "# job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "# bq_client.load_table_from_dataframe(df_rollback, BQ_TABLE_ID, job_config=job_config)\n",
        "\n",
        "# Or restore from backup:\n",
        "# backup_path = \"gs://BACKUP_BUCKET/BACKUP_PREFIX/TIMESTAMP/*.csv\"\n",
        "# df_backup = pd.read_csv(backup_path)\n",
        "# job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "# bq_client.load_table_from_dataframe(df_backup, BQ_TABLE_ID, job_config=job_config)\n",
        "```\n",
        "\n",
        "The backup location was printed in the backup cell above.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gcloud",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
