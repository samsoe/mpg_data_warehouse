{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Update gridVeg Image Metadata in BigQuery\n",
        "\n",
        "This notebook appends new image metadata records to the BigQuery table from a CSV file stored in GCS.\n",
        "\n",
        "**Operation**: APPEND new rows (not replace entire table)\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, google-cloud-storage, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n",
        "\n",
        "**TODO**: Add configuration section to config.yml for this table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values for gridVeg image metadata\n",
        "# TODO: Update these config keys once added to config.yml\n",
        "GCS_CSV_URL = config['gridveg_image_metadata']['gcs']['csv_url']\n",
        "BACKUP_BUCKET = config['gridveg_image_metadata']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gridveg_image_metadata']['gcs'].get('backup_prefix', 'backups/gridveg_image_metadata')\n",
        "BQ_TABLE_ID = config['gridveg_image_metadata']['bigquery']['table_id']\n",
        "BQ_PROJECT = config['gridveg_image_metadata']['bigquery'].get('project')\n",
        "\n",
        "# Verify required config values\n",
        "if not GCS_CSV_URL or GCS_CSV_URL.startswith('gs://your-'):\n",
        "    raise ValueError(\"Please configure gridveg_image_metadata.gcs.csv_url in config.yml\")\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure gridveg_image_metadata.bigquery.table_id in config.yml\")\n",
        "\n",
        "print(\"✓ Configuration loaded successfully\")\n",
        "print(f\"  CSV URL: {GCS_CSV_URL[:60]}...\" if len(GCS_CSV_URL) > 60 else f\"  CSV URL: {GCS_CSV_URL}\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "storage_client = storage.Client(project=BQ_PROJECT) if BQ_PROJECT else storage.Client()\n",
        "\n",
        "print(f\"✓ Clients initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load CSV Data from GCS\n",
        "\n",
        "Read the source CSV file containing new image metadata records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV from GCS (new data)\n",
        "print(\"Reading CSV from GCS...\")\n",
        "df_new = pd.read_csv(GCS_CSV_URL)\n",
        "\n",
        "print(f\"✓ CSV loaded successfully:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {list(df_new.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_new.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform CSV Data\n",
        "\n",
        "Apply schema transformations to match BigQuery table:\n",
        "- Rename columns to match destination schema\n",
        "- Convert date format from mm/dd/yy to ISO format (YYYY-MM-DD)\n",
        "- Add image_url column (initially NULL - to be populated separately)\n",
        "- Clean up Direction field (handle invisible character issue in \"North\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column mapping from CSV to BigQuery\n",
        "column_mapping = {\n",
        "    '__kp_Photos': 'image_ID',\n",
        "    'Survey Data::__kp_Survey': 'survey_ID',\n",
        "    'Survey Data::SurveyDate': 'date',\n",
        "    'Survey Data::SurveyYear': 'year',\n",
        "    'Survey Data::_kf_Site': 'grid_point',\n",
        "    'Direction': 'image_direction'\n",
        "}\n",
        "\n",
        "print(\"Column mapping:\")\n",
        "for csv_col, bq_col in column_mapping.items():\n",
        "    print(f\"  {csv_col:35s} → {bq_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify CSV columns match expected schema\n",
        "expected_csv_columns = set(column_mapping.keys())\n",
        "actual_csv_columns = set(df_new.columns)\n",
        "\n",
        "if actual_csv_columns == expected_csv_columns:\n",
        "    print(\"✓ CSV columns match expected schema\")\n",
        "else:\n",
        "    print(\"⚠ CSV column differences detected:\")\n",
        "    if actual_csv_columns - expected_csv_columns:\n",
        "        print(f\"  Unexpected columns: {actual_csv_columns - expected_csv_columns}\")\n",
        "    if expected_csv_columns - actual_csv_columns:\n",
        "        print(f\"  Missing columns: {expected_csv_columns - actual_csv_columns}\")\n",
        "    \n",
        "print(f\"\\nCSV columns: {list(df_new.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply transformation: rename columns\n",
        "df_transformed = df_new.copy()\n",
        "df_transformed = df_transformed.rename(columns=column_mapping)\n",
        "\n",
        "print(\"✓ Columns renamed\")\n",
        "print(f\"  Transformed columns: {list(df_transformed.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert date from m/d/yy to proper datetime/date format\n",
        "# Explicitly specify format to avoid parsing warnings and ensure consistency\n",
        "# Note: %y handles 2-digit years (00-68 = 2000-2068, 69-99 = 1969-1999)\n",
        "df_transformed['date'] = pd.to_datetime(df_transformed['date'], format='%m/%d/%y').dt.date\n",
        "\n",
        "print(\"✓ Date format converted to date type\")\n",
        "print(f\"  Sample dates: {df_transformed['date'].head().tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Direction field - strip whitespace and handle invisible characters\n",
        "# The source mentions \"invisible difference in North\" that displays as two levels\n",
        "df_transformed['image_direction'] = df_transformed['image_direction'].str.strip()\n",
        "\n",
        "# Check for unique values and any issues\n",
        "print(\"✓ Direction field cleaned\")\n",
        "print(f\"  Unique directions: {sorted(df_transformed['image_direction'].dropna().unique())}\")\n",
        "print(f\"  Direction counts:\")\n",
        "for direction, count in df_transformed['image_direction'].value_counts().items():\n",
        "    print(f\"    {repr(direction):12s}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add image_url column (not in source CSV - will be NULL initially)\n",
        "# This column exists in the destination schema but not in the source data\n",
        "df_transformed['image_url'] = None\n",
        "\n",
        "print(\"✓ Added image_url column (initially NULL)\")\n",
        "print(f\"  Column will be populated separately with actual image URLs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reorder columns to match destination schema\n",
        "expected_column_order = ['image_ID', 'image_url', 'survey_ID', 'date', 'year', 'grid_point', 'image_direction']\n",
        "df_transformed = df_transformed[expected_column_order]\n",
        "\n",
        "print(\"✓ Columns reordered to match destination schema\")\n",
        "print(f\"  Final columns: {list(df_transformed.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display transformed data info\n",
        "print(\"Transformed Data Info:\")\n",
        "df_transformed.info()\n",
        "print(f\"\\nTransformed data preview:\")\n",
        "df_transformed.head()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
