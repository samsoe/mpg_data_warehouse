{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Update gridVeg Image Metadata in BigQuery\n",
        "\n",
        "This notebook appends new image metadata records to the BigQuery table from a CSV file stored in GCS.\n",
        "\n",
        "**Operation**: APPEND new rows (not replace entire table)\n",
        "\n",
        "## Requirements\n",
        "- Google Cloud credentials configured\n",
        "- Configuration file: copy `config.example.yml` to `config.yml` and fill in your values\n",
        "- Required packages: google-cloud-bigquery, google-cloud-storage, pandas, pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n",
        "\n",
        "**TODO**: Add configuration section to config.yml for this table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path(\"../config.yml\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Configuration file not found: {config_path}\\n\"\n",
        "        \"Please copy config.example.yml to config.yml and fill in your values.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Extract configuration values for gridVeg image metadata\n",
        "# TODO: Update these config keys once added to config.yml\n",
        "GCS_CSV_URL = config['gridveg_image_metadata']['gcs']['csv_url']\n",
        "BACKUP_BUCKET = config['gridveg_image_metadata']['gcs'].get('backup_bucket')\n",
        "BACKUP_PREFIX = config['gridveg_image_metadata']['gcs'].get('backup_prefix', 'backups/gridveg_image_metadata')\n",
        "BQ_TABLE_ID = config['gridveg_image_metadata']['bigquery']['table_id']\n",
        "BQ_PROJECT = config['gridveg_image_metadata']['bigquery'].get('project')\n",
        "\n",
        "# Verify required config values\n",
        "if not GCS_CSV_URL or GCS_CSV_URL.startswith('gs://your-'):\n",
        "    raise ValueError(\"Please configure gridveg_image_metadata.gcs.csv_url in config.yml\")\n",
        "if not BQ_TABLE_ID or 'your-project' in BQ_TABLE_ID:\n",
        "    raise ValueError(\"Please configure gridveg_image_metadata.bigquery.table_id in config.yml\")\n",
        "\n",
        "print(\"✓ Configuration loaded successfully\")\n",
        "print(f\"  CSV URL: {GCS_CSV_URL[:60]}...\" if len(GCS_CSV_URL) > 60 else f\"  CSV URL: {GCS_CSV_URL}\")\n",
        "print(f\"  Table ID: {BQ_TABLE_ID}\")\n",
        "print(f\"  Backup: gs://{BACKUP_BUCKET}/{BACKUP_PREFIX}\" if BACKUP_BUCKET else \"  Backup: Not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=BQ_PROJECT) if BQ_PROJECT else bigquery.Client()\n",
        "storage_client = storage.Client(project=BQ_PROJECT) if BQ_PROJECT else storage.Client()\n",
        "\n",
        "print(f\"✓ Clients initialized\")\n",
        "print(f\"  Project: {bq_client.project}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load CSV Data from GCS\n",
        "\n",
        "Read the source CSV file containing new image metadata records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV from GCS (new data)\n",
        "print(\"Reading CSV from GCS...\")\n",
        "df_new = pd.read_csv(GCS_CSV_URL)\n",
        "\n",
        "print(f\"✓ CSV loaded successfully:\")\n",
        "print(f\"  Rows: {len(df_new)}\")\n",
        "print(f\"  Columns: {list(df_new.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_new.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform CSV Data\n",
        "\n",
        "Apply schema transformations to match BigQuery table:\n",
        "- Rename columns to match destination schema\n",
        "- Convert date format from mm/dd/yy to ISO format (YYYY-MM-DD)\n",
        "- Add image_url column (initially NULL - to be populated separately)\n",
        "- Clean up Direction field (handle invisible character issue in \"North\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column mapping from CSV to BigQuery\n",
        "column_mapping = {\n",
        "    '__kp_Photos': 'image_ID',\n",
        "    'Survey Data::__kp_Survey': 'survey_ID',\n",
        "    'Survey Data::SurveyDate': 'date',\n",
        "    'Survey Data::SurveyYear': 'year',\n",
        "    'Survey Data::_kf_Site': 'grid_point',\n",
        "    'Direction': 'image_direction'\n",
        "}\n",
        "\n",
        "print(\"Column mapping:\")\n",
        "for csv_col, bq_col in column_mapping.items():\n",
        "    print(f\"  {csv_col:35s} → {bq_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify CSV columns match expected schema\n",
        "expected_csv_columns = set(column_mapping.keys())\n",
        "actual_csv_columns = set(df_new.columns)\n",
        "\n",
        "if actual_csv_columns == expected_csv_columns:\n",
        "    print(\"✓ CSV columns match expected schema\")\n",
        "else:\n",
        "    print(\"⚠ CSV column differences detected:\")\n",
        "    if actual_csv_columns - expected_csv_columns:\n",
        "        print(f\"  Unexpected columns: {actual_csv_columns - expected_csv_columns}\")\n",
        "    if expected_csv_columns - actual_csv_columns:\n",
        "        print(f\"  Missing columns: {expected_csv_columns - actual_csv_columns}\")\n",
        "    \n",
        "print(f\"\\nCSV columns: {list(df_new.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply transformation: rename columns\n",
        "df_transformed = df_new.copy()\n",
        "df_transformed = df_transformed.rename(columns=column_mapping)\n",
        "\n",
        "print(\"✓ Columns renamed\")\n",
        "print(f\"  Transformed columns: {list(df_transformed.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert date from m/d/yy to proper datetime/date format\n",
        "# Explicitly specify format to avoid parsing warnings and ensure consistency\n",
        "# Note: %y handles 2-digit years (00-68 = 2000-2068, 69-99 = 1969-1999)\n",
        "df_transformed['date'] = pd.to_datetime(df_transformed['date'], format='%m/%d/%y').dt.date\n",
        "\n",
        "print(\"✓ Date format converted to date type\")\n",
        "print(f\"  Sample dates: {df_transformed['date'].head().tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Direction field - strip whitespace and handle invisible characters\n",
        "# The source mentions \"invisible difference in North\" that displays as two levels\n",
        "df_transformed['image_direction'] = df_transformed['image_direction'].str.strip()\n",
        "\n",
        "# Check for unique values and any issues\n",
        "print(\"✓ Direction field cleaned\")\n",
        "print(f\"  Unique directions: {sorted(df_transformed['image_direction'].dropna().unique())}\")\n",
        "print(f\"  Direction counts:\")\n",
        "for direction, count in df_transformed['image_direction'].value_counts().items():\n",
        "    print(f\"    {repr(direction):12s}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add image_url column (not in source CSV - will be NULL initially)\n",
        "# This column exists in the destination schema but not in the source data\n",
        "df_transformed['image_url'] = None\n",
        "\n",
        "print(\"✓ Added image_url column (initially NULL)\")\n",
        "print(f\"  Column will be populated separately with actual image URLs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reorder columns to match destination schema\n",
        "expected_column_order = ['image_ID', 'image_url', 'survey_ID', 'date', 'year', 'grid_point', 'image_direction']\n",
        "df_transformed = df_transformed[expected_column_order]\n",
        "\n",
        "print(\"✓ Columns reordered to match destination schema\")\n",
        "print(f\"  Final columns: {list(df_transformed.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display transformed data info\n",
        "print(\"Transformed Data Info:\")\n",
        "df_transformed.info()\n",
        "print(f\"\\nTransformed data preview:\")\n",
        "df_transformed.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Existing BigQuery Table\n",
        "\n",
        "Load the current data from BigQuery to compare with the new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read existing data from BigQuery\n",
        "print(f\"Reading existing data from {BQ_TABLE_ID}...\")\n",
        "query = f\"SELECT * FROM `{BQ_TABLE_ID}`\"\n",
        "\n",
        "try:\n",
        "    df_existing = bq_client.query(query).to_dataframe()\n",
        "    print(f\"✓ Existing table loaded:\")\n",
        "    print(f\"  Rows: {len(df_existing)}\")\n",
        "    print(f\"  Columns: {list(df_existing.columns)}\")\n",
        "    print(f\"\\nExisting data preview:\")\n",
        "    display(df_existing.head())\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error reading table: {e}\")\n",
        "    print(\"  This may be expected if the table doesn't exist yet.\")\n",
        "    df_existing = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display existing data info (if available)\n",
        "if df_existing is not None:\n",
        "    print(\"Existing Data Info:\")\n",
        "    df_existing.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare New vs Existing Data\n",
        "\n",
        "Identify which rows in the new data are not already in the existing table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare datasets\n",
        "if df_existing is not None:\n",
        "    print(\"=== Comparison Summary ===\\n\")\n",
        "    \n",
        "    # Row count comparison\n",
        "    print(f\"Row count:\")\n",
        "    print(f\"  Existing: {len(df_existing)}\")\n",
        "    print(f\"  New CSV:  {len(df_transformed)}\")\n",
        "    \n",
        "    # Column comparison\n",
        "    existing_cols = set(df_existing.columns)\n",
        "    new_cols = set(df_transformed.columns)\n",
        "    \n",
        "    if existing_cols == new_cols:\n",
        "        print(f\"\\n✓ Columns match ({len(new_cols)} columns)\")\n",
        "    else:\n",
        "        print(\"\\n⚠ Column differences detected:\")\n",
        "        if new_cols - existing_cols:\n",
        "            print(f\"  New columns: {new_cols - existing_cols}\")\n",
        "        if existing_cols - new_cols:\n",
        "            print(f\"  Missing columns: {existing_cols - new_cols}\")\n",
        "    \n",
        "    print(f\"\\nColumns: {list(df_transformed.columns)}\")\n",
        "else:\n",
        "    print(\"No existing data to compare - this will be a new table creation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify new records (not in existing table)\n",
        "# Use image_ID as the unique key (described as unique in source schema)\n",
        "if df_existing is not None:\n",
        "    existing_ids = set(df_existing['image_ID'])\n",
        "    new_ids = set(df_transformed['image_ID'])\n",
        "    \n",
        "    # Find records in new data that aren't in existing\n",
        "    ids_to_append = new_ids - existing_ids\n",
        "    \n",
        "    if ids_to_append:\n",
        "        df_to_append = df_transformed[df_transformed['image_ID'].isin(ids_to_append)].copy()\n",
        "        \n",
        "        print(f\"✓ Found {len(df_to_append)} new records to append\")\n",
        "        \n",
        "        # Show year breakdown of new records\n",
        "        print(f\"\\nNew records by year:\")\n",
        "        year_counts = df_to_append['year'].value_counts().sort_index()\n",
        "        for year, count in year_counts.items():\n",
        "            print(f\"  {year}: {count} records\")\n",
        "        \n",
        "        # Show direction breakdown\n",
        "        print(f\"\\nNew records by direction:\")\n",
        "        direction_counts = df_to_append['image_direction'].value_counts()\n",
        "        for direction, count in direction_counts.items():\n",
        "            print(f\"  {direction}: {count} records\")\n",
        "        \n",
        "        print(f\"\\nSample of new records:\")\n",
        "        display(df_to_append.head(10))\n",
        "    else:\n",
        "        df_to_append = None\n",
        "        print(\"⚠ No new records found - all records already exist in table\")\n",
        "        print(\"  Nothing to append.\")\n",
        "    \n",
        "    # Check for any duplicates\n",
        "    duplicate_ids = existing_ids & new_ids\n",
        "    if duplicate_ids:\n",
        "        print(f\"\\n⚠ Warning: {len(duplicate_ids)} records already exist in table\")\n",
        "        print(f\"  These will be skipped during append.\")\n",
        "        if len(duplicate_ids) <= 10:\n",
        "            print(f\"\\n  Sample duplicate image_IDs:\")\n",
        "            for img_id in list(duplicate_ids)[:10]:\n",
        "                print(f\"    {img_id}\")\n",
        "else:\n",
        "    # No existing table, so all records are new\n",
        "    df_to_append = df_transformed.copy()\n",
        "    print(f\"✓ No existing table - will create new table with {len(df_to_append)} records\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
